{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Large-Scale Data Engineering for AI\n",
    "## The Data Engineering Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and requirements\n",
    "import os\n",
    "import datetime\n",
    "import kaggle\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import logging\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lit, when, regexp_replace, trim, lower, upper, to_date, year, month, dayofmonth, explode, split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_spark_session(app_name=\"Spotify_ETL\"):\n",
    "    \"\"\"\n",
    "    Creates and returns a Spark session.\n",
    "    \n",
    "    Args:\n",
    "        app_name (str): The name of the Spark application.\n",
    "        \n",
    "    Returns:\n",
    "        SparkSession: A configured Spark session.\n",
    "    \"\"\"\n",
    "    logger.info(f\"Creating Spark session with app name: {app_name}\")\n",
    "    \n",
    "    try:\n",
    "        # Create a SparkSession with appropriate settings\n",
    "        spark = (SparkSession.builder\n",
    "                .appName(app_name)\n",
    "                .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "                .config(\"spark.sql.shuffle.partitions\", \"10\")\n",
    "                .config(\"spark.driver.memory\", \"2g\")\n",
    "                .config(\"spark.executor.memory\", \"4g\")\n",
    "                .config(\"spark.default.parallelism\", \"4\")\n",
    "                .config(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "                .getOrCreate())\n",
    "        \n",
    "        # Set log level to ERROR to reduce verbosity\n",
    "        spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "        \n",
    "        logger.info(\"Spark session created successfully\")\n",
    "        return spark\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error creating Spark session: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-12 12:44:16,129 - __main__ - INFO - Creating Spark session with app name: Spotify_ETL\n",
      "2025-04-12 12:44:21,013 - __main__ - INFO - Spark session created successfully\n",
      "2025-04-12 12:44:21,013 - __main__ - INFO - Spark session created successfully\n"
     ]
    }
   ],
   "source": [
    "\n",
    "spark = create_spark_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Landing Zone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base directory and Landing Zone directory structure\n",
    "BASE_DIR = \"../../data\"\n",
    "LANDING_ZONE_DIR = os.path.join(BASE_DIR, \"landing_zone\")\n",
    "os.makedirs(LANDING_ZONE_DIR, exist_ok=True)\n",
    "\n",
    "kaggle.api.authenticate()\n",
    "\n",
    "# List of datasets to ingest from Kaggle\n",
    "datasets = [\n",
    "        {\n",
    "            \"kaggle_id\": \"asaniczka/top-spotify-songs-in-73-countries-daily-updated\",\n",
    "            \"dataset_name\": \"top-spotify-songs-by-country\",\n",
    "            \"update\": True\n",
    "        },\n",
    "        {\n",
    "            \"kaggle_id\": \"maharshipandya/-spotify-tracks-dataset\",\n",
    "            \"dataset_name\": \"spotify-tracks-dataset\",\n",
    "            \"update\": False\n",
    "        },\n",
    "        {\n",
    "            \"kaggle_id\": \"terminate9298/songs-lyrics\",\n",
    "            \"dataset_name\": \"songs-lyrics\",\n",
    "            \"update\": False\n",
    "        }\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-12 12:45:53,082 - root - INFO - Starting the creation of the Landing Zone using Kaggle API\n",
      "2025-04-12 12:45:53,083 - root - INFO - Downloading dataset: asaniczka/top-spotify-songs-in-73-countries-daily-updated\n",
      "2025-04-12 12:45:53,083 - root - INFO - Downloading dataset: asaniczka/top-spotify-songs-in-73-countries-daily-updated\n",
      "2025-04-12 12:46:08,296 - root - INFO - CSV 'universal_top_spotify_songs.csv' converted to single Parquet file and saved as '../../data\\landing_zone\\top-spotify-songs-by-country.parquet'.\n",
      "2025-04-12 12:46:08,296 - root - INFO - CSV 'universal_top_spotify_songs.csv' converted to single Parquet file and saved as '../../data\\landing_zone\\top-spotify-songs-by-country.parquet'.\n",
      "2025-04-12 12:46:08,354 - root - INFO - Dataset 'top-spotify-songs-by-country' downloaded successfully.\n",
      "2025-04-12 12:46:08,354 - root - INFO - Dataset 'top-spotify-songs-by-country' downloaded successfully.\n",
      "2025-04-12 12:46:08,404 - root - INFO - Dataset 'top-spotify-songs-by-country' processed successfully.\n",
      "2025-04-12 12:46:08,406 - root - INFO - Downloading dataset: maharshipandya/-spotify-tracks-dataset\n",
      "2025-04-12 12:46:08,404 - root - INFO - Dataset 'top-spotify-songs-by-country' processed successfully.\n",
      "2025-04-12 12:46:08,406 - root - INFO - Downloading dataset: maharshipandya/-spotify-tracks-dataset\n",
      "2025-04-12 12:46:10,215 - root - INFO - CSV 'dataset.csv' converted to single Parquet file and saved as '../../data\\landing_zone\\spotify-tracks-dataset.parquet'.\n",
      "2025-04-12 12:46:10,219 - root - INFO - Dataset 'spotify-tracks-dataset' downloaded successfully.\n",
      "2025-04-12 12:46:10,215 - root - INFO - CSV 'dataset.csv' converted to single Parquet file and saved as '../../data\\landing_zone\\spotify-tracks-dataset.parquet'.\n",
      "2025-04-12 12:46:10,219 - root - INFO - Dataset 'spotify-tracks-dataset' downloaded successfully.\n",
      "2025-04-12 12:46:10,227 - root - INFO - Dataset 'spotify-tracks-dataset' processed successfully.\n",
      "2025-04-12 12:46:10,229 - root - INFO - Downloading dataset: terminate9298/songs-lyrics\n",
      "2025-04-12 12:46:10,227 - root - INFO - Dataset 'spotify-tracks-dataset' processed successfully.\n",
      "2025-04-12 12:46:10,229 - root - INFO - Downloading dataset: terminate9298/songs-lyrics\n",
      "2025-04-12 12:46:12,234 - root - INFO - CSV 'lyrics.csv' converted to single Parquet file and saved as '../../data\\landing_zone\\songs-lyrics.parquet'.\n",
      "2025-04-12 12:46:12,240 - root - INFO - Dataset 'songs-lyrics' downloaded successfully.\n",
      "2025-04-12 12:46:12,234 - root - INFO - CSV 'lyrics.csv' converted to single Parquet file and saved as '../../data\\landing_zone\\songs-lyrics.parquet'.\n",
      "2025-04-12 12:46:12,240 - root - INFO - Dataset 'songs-lyrics' downloaded successfully.\n",
      "2025-04-12 12:46:12,244 - root - INFO - Dataset 'songs-lyrics' processed successfully.\n",
      "2025-04-12 12:46:12,244 - root - INFO - All datasets have been processed.\n",
      "2025-04-12 12:46:12,245 - root - INFO - Landing Zone creation completed.\n",
      "2025-04-12 12:46:12,244 - root - INFO - Dataset 'songs-lyrics' processed successfully.\n",
      "2025-04-12 12:46:12,244 - root - INFO - All datasets have been processed.\n",
      "2025-04-12 12:46:12,245 - root - INFO - Landing Zone creation completed.\n"
     ]
    }
   ],
   "source": [
    "def data_collector_kaggle(kaggle_dataset: dict) -> None:\n",
    "    \"\"\"\n",
    "    Downloads a dataset from Kaggle and saves it to the landing zone.\n",
    "\n",
    "    Parameters:\n",
    "    kaggle_dataset (dict): A dictionary containing the Kaggle dataset information.\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract dataset information\n",
    "    kaggle_id = kaggle_dataset[\"kaggle_id\"]\n",
    "    dataset_name = kaggle_dataset[\"dataset_name\"]\n",
    "\n",
    "    # Create a temporary directory for the dataset using the actual dataset name\n",
    "    dataset_folder = os.path.join(LANDING_ZONE_DIR, f\"temp_{dataset_name}\")\n",
    "    os.makedirs(dataset_folder, exist_ok=True)\n",
    "\n",
    "    try:  \n",
    "        logging.info(f\"Downloading dataset: {kaggle_id}\")\n",
    "\n",
    "        kaggle.api.dataset_download_files(\n",
    "            kaggle_id,\n",
    "            path=dataset_folder,\n",
    "            unzip=True\n",
    "        )\n",
    "\n",
    "        csv_found = False\n",
    "        for filename in os.listdir(dataset_folder):\n",
    "            if filename in ['songs_details.csv', 'album_details.csv']:\n",
    "                continue\n",
    "            if filename.endswith(\".csv\"):\n",
    "                csv_found = True\n",
    "                csv_path = os.path.join(dataset_folder, filename)\n",
    "\n",
    "                # Read CSV with Pandas\n",
    "                df = pd.read_csv(csv_path)\n",
    "\n",
    "                # Write as a single Parquet file\n",
    "                final_path = os.path.join(LANDING_ZONE_DIR, f\"{dataset_name}.parquet\")\n",
    "                df.to_parquet(final_path, index=False)\n",
    "\n",
    "                logging.info(f\"CSV '{filename}' converted to single Parquet file and saved as '{final_path}'.\")\n",
    "                \n",
    "        if not csv_found:\n",
    "            logging.info(f\"No CSV file found in the downloaded dataset. Check the contents of the download.\")\n",
    "            \n",
    "        # Remove the temporary dataset folder\n",
    "        shutil.rmtree(dataset_folder)\n",
    "\n",
    "    except Exception as e:\n",
    "         # Remove the dataset folder if it exists\n",
    "        if os.path.exists(dataset_folder):\n",
    "            shutil.rmtree(dataset_folder)\n",
    "\n",
    "        # Log the error\n",
    "        logging.error(f\"Error downloading dataset '{kaggle_id}': {e}\")\n",
    "       \n",
    "        return\n",
    "\n",
    "    # Log the successful download\n",
    "    logging.info(f\"Dataset '{dataset_name}' downloaded successfully.\")\n",
    "\n",
    "def download_and_store_datasets(update: bool = False) -> None:\n",
    "    \"\"\"\n",
    "    Downloads and stores datasets from Kaggle into the landing zone.\n",
    "    \"\"\"\n",
    "    logging.info(\"Starting the creation of the Landing Zone using Kaggle API\")\n",
    "\n",
    "    for kaggle_dataset in datasets:\n",
    "        if update and not kaggle_dataset[\"update\"]:\n",
    "            logging.info(f\"Skipping dataset '{kaggle_dataset['dataset_name']}' as update is set to False.\")\n",
    "            continue\n",
    "        try:\n",
    "            dataset_name = kaggle_dataset[\"dataset_name\"]\n",
    "            data_collector_kaggle(kaggle_dataset)\n",
    "            logging.info(f\"Dataset '{dataset_name}' processed successfully.\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error processing dataset '{dataset_name}': {e}\")\n",
    "\n",
    "    logging.info(\"All datasets have been processed.\")\n",
    "    logging.info(\"Landing Zone creation completed.\")\n",
    "\n",
    "download_and_store_datasets(update=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formatted Zone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_spotify_tracks(spark, input_path, output_path):\n",
    "    \"\"\"\n",
    "    Process the Spotify tracks dataset.\n",
    "    \n",
    "    Args:\n",
    "        spark (SparkSession): The Spark session.\n",
    "        input_path (str): The input file path.\n",
    "        output_path (str): The output directory path.\n",
    "    \"\"\"\n",
    "    logger.info(f\"Processing Spotify tracks dataset from {input_path}\")\n",
    "    \n",
    "    try:\n",
    "        # Read the parquet file\n",
    "        df = spark.read.parquet(input_path)\n",
    "        \n",
    "        # Print schema and count before processing\n",
    "        logger.info(\"Original schema:\")\n",
    "        df.printSchema()\n",
    "        count_before = df.count()\n",
    "        logger.info(f\"Count before processing: {count_before}\")\n",
    "        \n",
    "        # Clean and transform the data\n",
    "        processed_df = df.select(\n",
    "            col(\"track_id\").alias(\"track_id\"),\n",
    "            col(\"track_name\").alias(\"track_name\"),\n",
    "            col(\"artists\").alias(\"artist_name\"),\n",
    "            col(\"album_name\"),\n",
    "            col(\"popularity\").cast(\"integer\").alias(\"popularity\"),\n",
    "            col(\"duration_ms\").cast(\"long\").alias(\"duration_ms\"),\n",
    "            col(\"explicit\").cast(\"boolean\").alias(\"explicit\"),\n",
    "            col(\"danceability\").cast(\"double\").alias(\"danceability\"),\n",
    "            col(\"energy\").cast(\"double\").alias(\"energy\"),\n",
    "            col(\"key\").cast(\"integer\").alias(\"key\"),\n",
    "            col(\"loudness\").cast(\"double\").alias(\"loudness\"),\n",
    "            col(\"mode\").cast(\"integer\").alias(\"mode\"),\n",
    "            col(\"speechiness\").cast(\"double\").alias(\"speechiness\"),\n",
    "            col(\"acousticness\").cast(\"double\").alias(\"acousticness\"),\n",
    "            col(\"instrumentalness\").cast(\"double\").alias(\"instrumentalness\"),\n",
    "            col(\"liveness\").cast(\"double\").alias(\"liveness\"),\n",
    "            col(\"valence\").cast(\"double\").alias(\"valence\"),\n",
    "            col(\"tempo\").cast(\"double\").alias(\"tempo\")\n",
    "        )\n",
    "        \n",
    "        # Remove rows with null track_id or track_name\n",
    "        # TRUSTED ZONE\n",
    "        # processed_df = processed_df.filter(\n",
    "        #     col(\"track_id\").isNotNull() & \n",
    "        #     col(\"track_name\").isNotNull()\n",
    "        # )\n",
    "        \n",
    "        # Print schema and count after processing\n",
    "        logger.info(\"Processed schema:\")\n",
    "        processed_df.printSchema()\n",
    "        count_after = processed_df.count()\n",
    "        logger.info(f\"Count after processing: {count_after}\")\n",
    "        logger.info(f\"Removed {count_before - count_after} rows during processing\")\n",
    "        \n",
    "        # Write the processed data as Parquet\n",
    "        processed_df.write.mode(\"overwrite\").parquet(output_path)\n",
    "        logger.info(f\"Processed Spotify tracks data saved to {output_path}\")\n",
    "        \n",
    "        return output_path\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing Spotify tracks dataset: {e}\")\n",
    "        raise\n",
    "\n",
    "def process_top_songs(spark, input_path, output_path):\n",
    "    \"\"\"\n",
    "    Procesa el dataset de Spotify con el siguiente schema:\n",
    "    \n",
    "    root\n",
    "     |-- spotify_id: string (nullable = true)\n",
    "     |-- name: string (nullable = true)\n",
    "     |-- artists: string (nullable = true)\n",
    "     |-- daily_rank: string (nullable = true)\n",
    "     |-- daily_movement: string (nullable = true)\n",
    "     |-- weekly_movement: string (nullable = true)\n",
    "     |-- country: string (nullable = true)\n",
    "     |-- snapshot_date: string (nullable = true)\n",
    "     |-- popularity: string (nullable = true)\n",
    "     |-- is_explicit: string (nullable = true)\n",
    "     |-- duration_ms: string (nullable = true)\n",
    "     |-- album_name: string (nullable = true)\n",
    "     |-- album_release_date: string (nullable = true)\n",
    "     |-- danceability: string (nullable = true)\n",
    "     |-- energy: string (nullable = true)\n",
    "     |-- key: string (nullable = true)\n",
    "     |-- loudness: string (nullable = true)\n",
    "     |-- mode: string (nullable = true)\n",
    "     |-- speechiness: string (nullable = true)\n",
    "     |-- acousticness: double (nullable = true)\n",
    "     |-- instrumentalness: double (nullable = true)\n",
    "     |-- liveness: double (nullable = true)\n",
    "     |-- valence: double (nullable = true)\n",
    "     |-- tempo: double (nullable = true)\n",
    "     |-- time_signature: double (nullable = true)\n",
    "    \n",
    "    Args:\n",
    "        spark (SparkSession): La sesión de Spark.\n",
    "        input_path (str): Ruta del archivo CSV de entrada.\n",
    "        output_path (str): Ruta del directorio de salida en HDFS.\n",
    "    \"\"\"\n",
    "    logger.info(f\"Procesando datos de Spotify desde {input_path}\")\n",
    "    \n",
    "    try:\n",
    "        # Leer el archivo parquet\n",
    "        df = spark.read.parquet(input_path)\n",
    "        \n",
    "        # Imprimir schema y contar filas antes del procesamiento\n",
    "        logger.info(\"Schema original:\")\n",
    "        df.printSchema()\n",
    "        count_before = df.count()\n",
    "        logger.info(f\"Filas antes del procesamiento: {count_before}\")\n",
    "        \n",
    "        # Seleccionar y transformar las columnas según el nuevo schema\n",
    "        processed_df = df.select(\n",
    "            col(\"spotify_id\").alias(\"spotify_id\"),\n",
    "            col(\"name\").alias(\"track_name\"),        \n",
    "            col(\"artists\").alias(\"artist_name\"),\n",
    "            col(\"daily_rank\").cast(\"integer\").alias(\"daily_rank\"),\n",
    "            col(\"daily_movement\").alias(\"daily_movement\"),\n",
    "            col(\"weekly_movement\").alias(\"weekly_movement\"),\n",
    "            col(\"country\").alias(\"country\"),\n",
    "            col(\"snapshot_date\").alias(\"snapshot_date\"),\n",
    "            col(\"popularity\").cast(\"integer\").alias(\"popularity\"),\n",
    "            col(\"is_explicit\").alias(\"is_explicit\"),\n",
    "            col(\"duration_ms\").cast(\"long\").alias(\"duration_ms\"),\n",
    "            col(\"album_name\").alias(\"album_name\"),\n",
    "            col(\"album_release_date\").alias(\"album_release_date\"),\n",
    "            col(\"danceability\").alias(\"danceability\"),\n",
    "            col(\"energy\").alias(\"energy\"),\n",
    "            col(\"key\").alias(\"key\"),\n",
    "            col(\"loudness\").alias(\"loudness\"),\n",
    "            col(\"mode\").alias(\"mode\"),\n",
    "            col(\"speechiness\").alias(\"speechiness\"),\n",
    "            col(\"acousticness\").cast(\"double\").alias(\"acousticness\"),\n",
    "            col(\"instrumentalness\").cast(\"double\").alias(\"instrumentalness\"),\n",
    "            col(\"liveness\").cast(\"double\").alias(\"liveness\"),\n",
    "            col(\"valence\").cast(\"double\").alias(\"valence\"),\n",
    "            col(\"tempo\").cast(\"double\").alias(\"tempo\"),\n",
    "            col(\"time_signature\").cast(\"double\").alias(\"time_signature\")\n",
    "        )\n",
    "        \n",
    "        # Manejo de valores nulos para algunas columnas numéricas\n",
    "        # TRUSTED ZONE\n",
    "        # processed_df = processed_df.na.fill({\n",
    "        #     \"daily_rank\": 0,\n",
    "        #     \"popularity\": 0,\n",
    "        #     \"duration_ms\": 0,\n",
    "        #     \"acousticness\": 0.0,\n",
    "        #     \"instrumentalness\": 0.0,\n",
    "        #     \"liveness\": 0.0,\n",
    "        #     \"valence\": 0.0,\n",
    "        #     \"tempo\": 0.0,\n",
    "        #     \"time_signature\": 0.0\n",
    "        # })\n",
    "        \n",
    "        # Convertir snapshot_date y album_release_date a formato fecha (ajustar el patrón si es necesario)\n",
    "        processed_df = processed_df.withColumn(\"snapshot_date\", to_date(col(\"snapshot_date\"), \"yyyy-MM-dd\"))\n",
    "        processed_df = processed_df.withColumn(\"album_release_date\", to_date(col(\"album_release_date\"), \"yyyy-MM-dd\"))\n",
    "        \n",
    "        # Extraer año, mes y día a partir de snapshot_date para posibles análisis adicionales\n",
    "        processed_df = processed_df.withColumn(\"snapshot_year\", year(col(\"snapshot_date\"))) \\\n",
    "                                   .withColumn(\"snapshot_month\", month(col(\"snapshot_date\"))) \\\n",
    "                                   .withColumn(\"snapshot_day\", dayofmonth(col(\"snapshot_date\")))\n",
    "        \n",
    "        # Limpiar el campo country: eliminar espacios extras y convertir a mayúsculas\n",
    "        processed_df = processed_df.withColumn(\n",
    "            \"country\", \n",
    "            upper(trim(regexp_replace(col(\"country\"), \"\\\\s+\", \" \")))\n",
    "        )\n",
    "        \n",
    "        # Filtrar filas donde el nombre de la canción (track_name) no sea nulo\n",
    "        # TRUSTED ZONE\n",
    "        # processed_df = processed_df.filter(col(\"track_name\").isNotNull())\n",
    "        \n",
    "        # Imprimir el schema y contar las filas después del procesamiento\n",
    "        logger.info(\"Schema procesado:\")\n",
    "        processed_df.printSchema()\n",
    "        count_after = processed_df.count()\n",
    "        logger.info(f\"Filas después del procesamiento: {count_after}\")\n",
    "        logger.info(f\"Se removieron {count_before - count_after} filas durante el procesamiento\")\n",
    "        \n",
    "        # Escribir el DataFrame procesado en formato Parquet en HDFS\n",
    "        processed_df.write.mode(\"overwrite\").parquet(output_path)\n",
    "        logger.info(f\"Datos procesados de Spotify guardados en {output_path}\")\n",
    "        \n",
    "        return output_path\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error al procesar los datos de Spotify: {e}\")\n",
    "        raise\n",
    "\n",
    "def process_song_lyrics(spark, input_path, output_path):\n",
    "    \"\"\"\n",
    "    Process the song lyrics dataset.\n",
    "    \n",
    "    Args:\n",
    "        spark (SparkSession): The Spark session.\n",
    "        input_path (str): The input file path.\n",
    "        output_path (str): The output directory path.\n",
    "    \"\"\"\n",
    "    logger.info(f\"Processing song lyrics from {input_path}\")\n",
    "    \n",
    "    try:\n",
    "        # Read the CSV file with proper encoding\n",
    "        df = spark.read.parquet(input_path)\n",
    "        \n",
    "        # Print schema and count before processing\n",
    "        logger.info(\"Original schema:\")\n",
    "        df.printSchema()\n",
    "        count_before = df.count()\n",
    "        logger.info(f\"Count before processing: {count_before}\")\n",
    "        \n",
    "        # Clean and transform the data based on the actual columns in songs-lyrics.csv\n",
    "        processed_df = df.select(\n",
    "            col(\"Unnamed: 0\").cast(\"integer\").alias(\"song_id\"),\n",
    "            col(\"artist\").alias(\"artist_name\"),\n",
    "            col(\"song_name\").alias(\"song_name\"),\n",
    "            col(\"lyrics\").alias(\"song_lyrics\")\n",
    "        )\n",
    "        \n",
    "        # Clean artist and track names\n",
    "        processed_df = processed_df.withColumn(\"artist_name\", trim(col(\"artist_name\"))) \\\n",
    "                                 .withColumn(\"song_name\", trim(col(\"song_name\")))\n",
    "        \n",
    "        # Filter rows with valid song_id and track_name\n",
    "        # TRUSTED ZONE\n",
    "        # processed_df = processed_df.filter(col(\"song_id\").isNotNull() & col(\"track_name\").isNotNull())\n",
    "        \n",
    "        # Print schema and count after processing\n",
    "        logger.info(\"Processed schema:\")\n",
    "        processed_df.printSchema()\n",
    "        count_after = processed_df.count()\n",
    "        logger.info(f\"Count after processing: {count_after}\")\n",
    "        logger.info(f\"Removed {count_before - count_after} rows during processing\")\n",
    "        \n",
    "        # Write the processed data as Parquet\n",
    "        processed_df.write.mode(\"overwrite\").parquet(output_path)\n",
    "        logger.info(f\"Processed song lyrics data saved to {output_path}\")\n",
    "        \n",
    "        return output_path\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing song lyrics dataset: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-12 12:54:32,291 - __main__ - INFO - Processing Spotify tracks dataset from ../../data/landing_zone/spotify-tracks-dataset.parquet\n",
      "2025-04-12 12:54:32,388 - __main__ - INFO - Original schema:\n",
      "2025-04-12 12:54:32,388 - __main__ - INFO - Original schema:\n",
      "2025-04-12 12:54:32,486 - __main__ - INFO - Count before processing: 114000\n",
      "2025-04-12 12:54:32,486 - __main__ - INFO - Count before processing: 114000\n",
      "2025-04-12 12:54:32,521 - __main__ - INFO - Processed schema:\n",
      "2025-04-12 12:54:32,521 - __main__ - INFO - Processed schema:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Unnamed: 0: long (nullable = true)\n",
      " |-- track_id: string (nullable = true)\n",
      " |-- artists: string (nullable = true)\n",
      " |-- album_name: string (nullable = true)\n",
      " |-- track_name: string (nullable = true)\n",
      " |-- popularity: long (nullable = true)\n",
      " |-- duration_ms: long (nullable = true)\n",
      " |-- explicit: boolean (nullable = true)\n",
      " |-- danceability: double (nullable = true)\n",
      " |-- energy: double (nullable = true)\n",
      " |-- key: long (nullable = true)\n",
      " |-- loudness: double (nullable = true)\n",
      " |-- mode: long (nullable = true)\n",
      " |-- speechiness: double (nullable = true)\n",
      " |-- acousticness: double (nullable = true)\n",
      " |-- instrumentalness: double (nullable = true)\n",
      " |-- liveness: double (nullable = true)\n",
      " |-- valence: double (nullable = true)\n",
      " |-- tempo: double (nullable = true)\n",
      " |-- time_signature: long (nullable = true)\n",
      " |-- track_genre: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- track_id: string (nullable = true)\n",
      " |-- track_name: string (nullable = true)\n",
      " |-- artist_name: string (nullable = true)\n",
      " |-- album_name: string (nullable = true)\n",
      " |-- popularity: integer (nullable = true)\n",
      " |-- duration_ms: long (nullable = true)\n",
      " |-- explicit: boolean (nullable = true)\n",
      " |-- danceability: double (nullable = true)\n",
      " |-- energy: double (nullable = true)\n",
      " |-- key: integer (nullable = true)\n",
      " |-- loudness: double (nullable = true)\n",
      " |-- mode: integer (nullable = true)\n",
      " |-- speechiness: double (nullable = true)\n",
      " |-- acousticness: double (nullable = true)\n",
      " |-- instrumentalness: double (nullable = true)\n",
      " |-- liveness: double (nullable = true)\n",
      " |-- valence: double (nullable = true)\n",
      " |-- tempo: double (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-12 12:54:32,614 - __main__ - INFO - Count after processing: 114000\n",
      "2025-04-12 12:54:32,616 - __main__ - INFO - Removed 0 rows during processing\n",
      "2025-04-12 12:54:32,616 - __main__ - INFO - Removed 0 rows during processing\n",
      "2025-04-12 12:54:33,555 - __main__ - INFO - Processed Spotify tracks data saved to ../../data/formatted_zone/spotify-tracks-dataset\n",
      "2025-04-12 12:54:33,556 - __main__ - INFO - Procesando datos de Spotify desde ../../data/landing_zone/top-spotify-songs-by-country.parquet\n",
      "2025-04-12 12:54:33,555 - __main__ - INFO - Processed Spotify tracks data saved to ../../data/formatted_zone/spotify-tracks-dataset\n",
      "2025-04-12 12:54:33,556 - __main__ - INFO - Procesando datos de Spotify desde ../../data/landing_zone/top-spotify-songs-by-country.parquet\n",
      "2025-04-12 12:54:33,625 - __main__ - INFO - Schema original:\n",
      "2025-04-12 12:54:33,625 - __main__ - INFO - Schema original:\n",
      "2025-04-12 12:54:33,757 - __main__ - INFO - Filas antes del procesamiento: 1919457\n",
      "2025-04-12 12:54:33,757 - __main__ - INFO - Filas antes del procesamiento: 1919457\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- spotify_id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- artists: string (nullable = true)\n",
      " |-- daily_rank: long (nullable = true)\n",
      " |-- daily_movement: long (nullable = true)\n",
      " |-- weekly_movement: long (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- snapshot_date: string (nullable = true)\n",
      " |-- popularity: long (nullable = true)\n",
      " |-- is_explicit: boolean (nullable = true)\n",
      " |-- duration_ms: long (nullable = true)\n",
      " |-- album_name: string (nullable = true)\n",
      " |-- album_release_date: string (nullable = true)\n",
      " |-- danceability: double (nullable = true)\n",
      " |-- energy: double (nullable = true)\n",
      " |-- key: long (nullable = true)\n",
      " |-- loudness: double (nullable = true)\n",
      " |-- mode: long (nullable = true)\n",
      " |-- speechiness: double (nullable = true)\n",
      " |-- acousticness: double (nullable = true)\n",
      " |-- instrumentalness: double (nullable = true)\n",
      " |-- liveness: double (nullable = true)\n",
      " |-- valence: double (nullable = true)\n",
      " |-- tempo: double (nullable = true)\n",
      " |-- time_signature: long (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-12 12:54:33,877 - __main__ - INFO - Schema procesado:\n",
      "2025-04-12 12:54:34,025 - __main__ - INFO - Filas después del procesamiento: 1919457\n",
      "2025-04-12 12:54:34,026 - __main__ - INFO - Se removieron 0 filas durante el procesamiento\n",
      "2025-04-12 12:54:34,025 - __main__ - INFO - Filas después del procesamiento: 1919457\n",
      "2025-04-12 12:54:34,026 - __main__ - INFO - Se removieron 0 filas durante el procesamiento\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- spotify_id: string (nullable = true)\n",
      " |-- track_name: string (nullable = true)\n",
      " |-- artist_name: string (nullable = true)\n",
      " |-- daily_rank: integer (nullable = true)\n",
      " |-- daily_movement: long (nullable = true)\n",
      " |-- weekly_movement: long (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- snapshot_date: date (nullable = true)\n",
      " |-- popularity: integer (nullable = true)\n",
      " |-- is_explicit: boolean (nullable = true)\n",
      " |-- duration_ms: long (nullable = true)\n",
      " |-- album_name: string (nullable = true)\n",
      " |-- album_release_date: date (nullable = true)\n",
      " |-- danceability: double (nullable = true)\n",
      " |-- energy: double (nullable = true)\n",
      " |-- key: long (nullable = true)\n",
      " |-- loudness: double (nullable = true)\n",
      " |-- mode: long (nullable = true)\n",
      " |-- speechiness: double (nullable = true)\n",
      " |-- acousticness: double (nullable = true)\n",
      " |-- instrumentalness: double (nullable = true)\n",
      " |-- liveness: double (nullable = true)\n",
      " |-- valence: double (nullable = true)\n",
      " |-- tempo: double (nullable = true)\n",
      " |-- time_signature: double (nullable = true)\n",
      " |-- snapshot_year: integer (nullable = true)\n",
      " |-- snapshot_month: integer (nullable = true)\n",
      " |-- snapshot_day: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-12 12:54:42,016 - __main__ - INFO - Datos procesados de Spotify guardados en ../../data/formatted_zone/top-spotify-songs-by-country\n",
      "2025-04-12 12:54:42,017 - __main__ - INFO - Processing song lyrics from ../../data/landing_zone/songs-lyrics.parquet\n",
      "2025-04-12 12:54:42,017 - __main__ - INFO - Processing song lyrics from ../../data/landing_zone/songs-lyrics.parquet\n",
      "2025-04-12 12:54:42,098 - __main__ - INFO - Original schema:\n",
      "2025-04-12 12:54:42,098 - __main__ - INFO - Original schema:\n",
      "2025-04-12 12:54:42,196 - __main__ - INFO - Count before processing: 25742\n",
      "2025-04-12 12:54:42,213 - __main__ - INFO - Processed schema:\n",
      "2025-04-12 12:54:42,196 - __main__ - INFO - Count before processing: 25742\n",
      "2025-04-12 12:54:42,213 - __main__ - INFO - Processed schema:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Unnamed: 0: long (nullable = true)\n",
      " |-- link: string (nullable = true)\n",
      " |-- artist: string (nullable = true)\n",
      " |-- song_name: string (nullable = true)\n",
      " |-- lyrics: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- song_id: integer (nullable = true)\n",
      " |-- artist_name: string (nullable = true)\n",
      " |-- song_name: string (nullable = true)\n",
      " |-- song_lyrics: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-12 12:54:42,305 - __main__ - INFO - Count after processing: 25742\n",
      "2025-04-12 12:54:42,306 - __main__ - INFO - Removed 0 rows during processing\n",
      "2025-04-12 12:54:42,306 - __main__ - INFO - Removed 0 rows during processing\n",
      "2025-04-12 12:54:42,749 - __main__ - INFO - Processed song lyrics data saved to ../../data/formatted_zone/songs-lyrics\n",
      "2025-04-12 12:54:42,749 - __main__ - INFO - Processed song lyrics data saved to ../../data/formatted_zone/songs-lyrics\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'../../data/formatted_zone/songs-lyrics'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define input and output paths\n",
    "input_paths = {\n",
    "    'spotify_tracks': '../../data/landing_zone/spotify-tracks-dataset.parquet',\n",
    "    'top_songs': '../../data/landing_zone/top-spotify-songs-by-country.parquet',\n",
    "    'song_lyrics': '../../data/landing_zone/songs-lyrics.parquet'\n",
    "}\n",
    "output_paths = {\n",
    "    'spotify_tracks': '../../data/formatted_zone/spotify-tracks-dataset',\n",
    "    'top_songs': '../../data/formatted_zone/top-spotify-songs-by-country',\n",
    "    'song_lyrics': '../../data/formatted_zone/songs-lyrics'\n",
    "}\n",
    "\n",
    "# Process datasets\n",
    "process_spotify_tracks(spark, input_paths['spotify_tracks'], output_paths['spotify_tracks'])\n",
    "process_top_songs(spark, input_paths['top_songs'], output_paths['top_songs'])\n",
    "process_song_lyrics(spark, input_paths['song_lyrics'], output_paths['song_lyrics'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Trusted Zone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Exploitation Zone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Data Analysis Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
