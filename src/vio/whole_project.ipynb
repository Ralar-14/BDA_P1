{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Large-Scale Data Engineering for AI\n",
    "Per a fer aquest projecte hem hagafat tres datasets relacionats amb música. Més concretament relacionats amb cançons que te Spotify. A continuació deixem una petita descripció de cadascun:\n",
    "- **song-lyrics** --> conté unes 25000 cançons amb les seves corresponents lletres.\n",
    "- **spotify-tracks-dataset** --> conté cançons d'Spotify de 125 gèneres diferents i altres atributs.\n",
    "- **top-spotify-songs-by-country** --> conté les cançons més populars de 72 països. S'actualitza diàriament.\n",
    "\n",
    "## The Data Engineering Pipeline\n",
    "### The Landing Zone\n",
    "Pel que fa a la landing zone, hem accedit a l'API de Kaggle per a obtenir els datasets. Un pas previ que s'ha de fer per a poder tenir accés és generar un token de Kaggle. Primer has d'entrar en el teu perfil i anar a la configuració, baixant es troba un apartat on hi diu \"Create New Token\" i d'allà es descarregarà un fitxer .json. Hem utilitzat la [guia](https://www.kaggle.com/docs/api#getting-started-installation-&-authentication) d'ús de l'API per aconseguir-ho. S'hauran de fer aquestes comandes:\n",
    "```\n",
    "pip install kaggle\n",
    "mkdir ~/.kaggle\n",
    "mv ~/Downloads/kaggle.json ~/.kaggle/kaggle.json\n",
    "```\n",
    "\n",
    "Un cop ja tenim accés a la API, hem fet una funció recol·lectora de dades per a baixar els tres datasets i guardar-los a l'adreça desitjada. Aquests datasets estan en format .csv i nosaltres hem preferit passar-los a format .parquet. A l'adreça del dataset de les lletres no només hi ha les lletres sinó que hi ha dos datasets més amb informació sobre els àlbums i més informació sobre les cançons. És per això que només ens hem de quedar amb el de lletres. En conclusió, ens baixem el dataset de l'API, el passem a .parquet, el guardem a la carpeta de \"data/landing_zone\". Al llarg d'aquesta execució, es van fent moltes comprovacions, en cas que algun moment falli es tingui proouta informació per trobar l'error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'child' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/.pyenv/versions/3.11.5/lib/python3.11/site-packages/IPython/utils/_process_posix.py:151\u001b[0m, in \u001b[0;36mProcessHandler.system\u001b[0;34m(self, cmd)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 151\u001b[0m     child \u001b[38;5;241m=\u001b[39m \u001b[43mpexpect\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspawn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m-c\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcmd\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Vanilla Pexpect\u001b[39;00m\n\u001b[1;32m    152\u001b[0m flush \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39mstdout\u001b[38;5;241m.\u001b[39mflush\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.5/lib/python3.11/site-packages/pexpect/pty_spawn.py:205\u001b[0m, in \u001b[0;36mspawn.__init__\u001b[0;34m(self, command, args, timeout, maxread, searchwindowsize, logfile, cwd, env, ignore_sighup, echo, preexec_fn, encoding, codec_errors, dimensions, use_poll)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_spawn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreexec_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdimensions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_poll \u001b[38;5;241m=\u001b[39m use_poll\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.5/lib/python3.11/site-packages/pexpect/pty_spawn.py:303\u001b[0m, in \u001b[0;36mspawn._spawn\u001b[0;34m(self, command, args, preexec_fn, dimensions)\u001b[0m\n\u001b[1;32m    300\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs \u001b[38;5;241m=\u001b[39m [a \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(a, \u001b[38;5;28mbytes\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m a\u001b[38;5;241m.\u001b[39mencode(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoding)\n\u001b[1;32m    301\u001b[0m                  \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs]\n\u001b[0;32m--> 303\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mptyproc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_spawnpty\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    304\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mcwd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcwd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mptyproc\u001b[38;5;241m.\u001b[39mpid\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.5/lib/python3.11/site-packages/pexpect/pty_spawn.py:315\u001b[0m, in \u001b[0;36mspawn._spawnpty\u001b[0;34m(self, args, **kwargs)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''Spawn a pty and return an instance of PtyProcess.'''\u001b[39;00m\n\u001b[0;32m--> 315\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mptyprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPtyProcess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspawn\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.5/lib/python3.11/site-packages/ptyprocess/ptyprocess.py:315\u001b[0m, in \u001b[0;36mPtyProcess.spawn\u001b[0;34m(cls, argv, cwd, env, echo, preexec_fn, dimensions, pass_fds)\u001b[0m\n\u001b[1;32m    314\u001b[0m os\u001b[38;5;241m.\u001b[39mclose(exec_err_pipe_write)\n\u001b[0;32m--> 315\u001b[0m exec_err_data \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexec_err_pipe_read\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4096\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    316\u001b[0m os\u001b[38;5;241m.\u001b[39mclose(exec_err_pipe_read)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Imports and requirements\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msystem\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpip install kaggle\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpip install pyspark\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.5/lib/python3.11/site-packages/ipykernel/zmqshell.py:657\u001b[0m, in \u001b[0;36mZMQInteractiveShell.system_piped\u001b[0;34m(self, cmd)\u001b[0m\n\u001b[1;32m    655\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muser_ns[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_exit_code\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m system(cmd)\n\u001b[1;32m    656\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 657\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muser_ns[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_exit_code\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43msystem\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvar_expand\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcmd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.5/lib/python3.11/site-packages/IPython/utils/_process_posix.py:167\u001b[0m, in \u001b[0;36mProcessHandler.system\u001b[0;34m(self, cmd)\u001b[0m\n\u001b[1;32m    162\u001b[0m         out_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(child\u001b[38;5;241m.\u001b[39mbefore)\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;66;03m# We need to send ^C to the process.  The ascii code for '^C' is 3\u001b[39;00m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;66;03m# (the character is known as ETX for 'End of Text', see\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;66;03m# curses.ascii.ETX).\u001b[39;00m\n\u001b[0;32m--> 167\u001b[0m     \u001b[43mchild\u001b[49m\u001b[38;5;241m.\u001b[39msendline(\u001b[38;5;28mchr\u001b[39m(\u001b[38;5;241m3\u001b[39m))\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;66;03m# Read and print any more output the program might produce on its\u001b[39;00m\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;66;03m# way out.\u001b[39;00m\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: cannot access local variable 'child' where it is not associated with a value"
     ]
    }
   ],
   "source": [
    "# Imports and requirements\n",
    "!pip install kaggle\n",
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and requirements\n",
    "import os\n",
    "import datetime\n",
    "import kaggle\n",
    "import pandas as pd\n",
    "import logging\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lit, when, regexp_replace, trim, lower, upper, to_date, year, month, dayofmonth, explode, split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /home/violeta/.config/kaggle/kaggle.json'\n"
     ]
    }
   ],
   "source": [
    "# Base directory and Landing Zone directory structure\n",
    "BASE_DIR = \"./data\"\n",
    "LANDING_ZONE_DIR = os.path.join(BASE_DIR, \"landing_zone\")\n",
    "os.makedirs(LANDING_ZONE_DIR, exist_ok=True)\n",
    "\n",
    "kaggle.api.authenticate()\n",
    "\n",
    "# List of datasets to ingest from Kaggle\n",
    "datasets = [\n",
    "        {\n",
    "            \"kaggle_id\": \"asaniczka/top-spotify-songs-in-73-countries-daily-updated\",\n",
    "            \"dataset_name\": \"top-spotify-songs-by-country\",\n",
    "            \"update\": True\n",
    "        },\n",
    "        {\n",
    "            \"kaggle_id\": \"maharshipandya/-spotify-tracks-dataset\",\n",
    "            \"dataset_name\": \"spotify-tracks-dataset\",\n",
    "            \"update\": False\n",
    "        },\n",
    "        {\n",
    "            \"kaggle_id\": \"terminate9298/songs-lyrics\",\n",
    "            \"dataset_name\": \"songs-lyrics\",\n",
    "            \"update\": False\n",
    "        }\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-12 14:59:39] Starting the creation of the Landing Zone using Kaggle API\n",
      "[2025-04-12 14:59:39] Downloading dataset: asaniczka/top-spotify-songs-in-73-countries-daily-updated\n",
      "Dataset URL: https://www.kaggle.com/datasets/asaniczka/top-spotify-songs-in-73-countries-daily-updated\n",
      "[2025-04-12 14:59:56] CSV 'universal_top_spotify_songs.csv' converted to single Parquet file and saved as './data/landing_zone/top-spotify-songs-by-country.parquet'.\n",
      "[2025-04-12 14:59:56] Dataset 'top-spotify-songs-by-country' downloaded successfully.\n",
      "[2025-04-12 14:59:56] Dataset 'top-spotify-songs-by-country' processed successfully.\n",
      "[2025-04-12 14:59:56] Downloading dataset: maharshipandya/-spotify-tracks-dataset\n",
      "Dataset URL: https://www.kaggle.com/datasets/maharshipandya/-spotify-tracks-dataset\n",
      "[2025-04-12 14:59:58] CSV 'dataset.csv' converted to single Parquet file and saved as './data/landing_zone/spotify-tracks-dataset.parquet'.\n",
      "[2025-04-12 14:59:58] Dataset 'spotify-tracks-dataset' downloaded successfully.\n",
      "[2025-04-12 14:59:58] Dataset 'spotify-tracks-dataset' processed successfully.\n",
      "[2025-04-12 14:59:58] Downloading dataset: terminate9298/songs-lyrics\n",
      "Dataset URL: https://www.kaggle.com/datasets/terminate9298/songs-lyrics\n",
      "[2025-04-12 15:00:01] CSV 'lyrics.csv' converted to single Parquet file and saved as './data/landing_zone/songs-lyrics.parquet'.\n",
      "[2025-04-12 15:00:01] Dataset 'songs-lyrics' downloaded successfully.\n",
      "[2025-04-12 15:00:01] Dataset 'songs-lyrics' processed successfully.\n",
      "[2025-04-12 15:00:01] All datasets have been processed.\n",
      "[2025-04-12 15:00:01] Landing Zone creation completed.\n"
     ]
    }
   ],
   "source": [
    "# Logging function to timestamp each message\n",
    "def log(message):\n",
    "    print(f\"[{datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] {message}\")\n",
    "\n",
    "def data_collector_kaggle(kaggle_dataset: dict) -> None:\n",
    "    \"\"\"\n",
    "    Downloads a dataset from Kaggle and saves it to the landing zone.\n",
    "\n",
    "    Parameters:\n",
    "    kaggle_dataset (dict): A dictionary containing the Kaggle dataset information.\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract dataset information\n",
    "    kaggle_id = kaggle_dataset[\"kaggle_id\"]\n",
    "    dataset_name = kaggle_dataset[\"dataset_name\"]\n",
    "    \n",
    "    try:  \n",
    "        log(f\"Downloading dataset: {kaggle_id}\")\n",
    "\n",
    "        kaggle.api.dataset_download_files(\n",
    "            kaggle_id,\n",
    "            path=LANDING_ZONE_DIR,\n",
    "            unzip=True\n",
    "        )\n",
    "\n",
    "        csv_found = False\n",
    "        for filename in os.listdir(LANDING_ZONE_DIR):\n",
    "            if filename in ['songs_details.csv', 'album_details.csv']:\n",
    "                csv_path = os.path.join(LANDING_ZONE_DIR, filename)\n",
    "                os.remove(csv_path)\n",
    "                continue\n",
    "            if filename.endswith(\".csv\"):\n",
    "                csv_found = True\n",
    "                csv_path = os.path.join(LANDING_ZONE_DIR, filename)\n",
    "\n",
    "                # Read CSV with Pandas\n",
    "                df = pd.read_csv(csv_path)\n",
    "\n",
    "                # Write Parquet\n",
    "                final_path = os.path.join(LANDING_ZONE_DIR, f\"{dataset_name}.parquet\")\n",
    "                df.to_parquet(final_path, index=False)\n",
    "\n",
    "                log(f\"CSV '{filename}' converted to single Parquet file and saved as '{final_path}'.\")\n",
    "\n",
    "                os.remove(csv_path)\n",
    "                        \n",
    "        if not csv_found:\n",
    "            log(f\"No CSV file found in the downloaded dataset. Check the contents of the download.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        # Log the error\n",
    "        log(f\"Error downloading dataset '{kaggle_id}': {e}\")\n",
    "        return\n",
    "\n",
    "    # Log the successful download\n",
    "    log(f\"Dataset '{dataset_name}' downloaded successfully.\")\n",
    "\n",
    "def download_and_store_datasets(update: bool = False) -> None:\n",
    "    \"\"\"\n",
    "    Downloads and stores datasets from Kaggle into the landing zone.\n",
    "    \"\"\"\n",
    "    log(\"Starting the creation of the Landing Zone using Kaggle API\")\n",
    "\n",
    "    for kaggle_dataset in datasets:\n",
    "        if update and not kaggle_dataset[\"update\"]:\n",
    "            log(f\"Skipping dataset '{kaggle_dataset['dataset_name']}' as update is set to False.\")\n",
    "            continue\n",
    "        try:\n",
    "            dataset_name = kaggle_dataset[\"dataset_name\"]\n",
    "            data_collector_kaggle(kaggle_dataset)\n",
    "            log(f\"Dataset '{dataset_name}' processed successfully.\")\n",
    "        except Exception as e:\n",
    "            log(f\"Error processing dataset '{dataset_name}': {e}\")\n",
    "\n",
    "    log(\"All datasets have been processed.\")\n",
    "    log(\"Landing Zone creation completed.\")\n",
    "\n",
    "download_and_store_datasets(update=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Formatted Zone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-12 15:00:01,077 - __main__ - INFO - Creating Spark session with app name: Spotify_ETL\n",
      "2025-04-12 15:00:01,100 - __main__ - INFO - Spark session created successfully\n"
     ]
    }
   ],
   "source": [
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def create_spark_session(app_name=\"Spotify_ETL\"):\n",
    "    \"\"\"\n",
    "    Creates and returns a Spark session.\n",
    "    \n",
    "    Args:\n",
    "        app_name (str): The name of the Spark application.\n",
    "        \n",
    "    Returns:\n",
    "        SparkSession: A configured Spark session.\n",
    "    \"\"\"\n",
    "    logger.info(f\"Creating Spark session with app name: {app_name}\")\n",
    "    \n",
    "    try:\n",
    "        # Create a SparkSession with appropriate settings\n",
    "        spark = (SparkSession.builder\n",
    "                .appName(app_name)\n",
    "                .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "                .config(\"spark.sql.shuffle.partitions\", \"10\")\n",
    "                .config(\"spark.driver.memory\", \"2g\")\n",
    "                .config(\"spark.executor.memory\", \"4g\")\n",
    "                .config(\"spark.default.parallelism\", \"4\")\n",
    "                .config(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "                .getOrCreate())\n",
    "        \n",
    "        # Set log level to ERROR to reduce verbosity\n",
    "        spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "        \n",
    "        logger.info(\"Spark session created successfully\")\n",
    "        return spark\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error creating Spark session: {e}\")\n",
    "        raise\n",
    "\n",
    "spark = create_spark_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_spotify_tracks(spark, input_path, output_path):\n",
    "    \"\"\"\n",
    "    Process the Spotify tracks dataset.\n",
    "    \n",
    "    Args:\n",
    "        spark (SparkSession): The Spark session.\n",
    "        input_path (str): The input file path.\n",
    "        output_path (str): The output directory path.\n",
    "    \"\"\"\n",
    "    logger.info(f\"Processing Spotify tracks dataset from {input_path}\")\n",
    "    \n",
    "    try:\n",
    "        # Read the parquet file\n",
    "        df = spark.read.parquet(input_path)\n",
    "        \n",
    "        # Print schema and count before processing\n",
    "        logger.info(\"Original schema:\")\n",
    "        df.printSchema()\n",
    "        count_before = df.count()\n",
    "        logger.info(f\"Count before processing: {count_before}\")\n",
    "        \n",
    "        # Clean and transform the data\n",
    "        processed_df = df.select(\n",
    "            col(\"track_id\").alias(\"track_id\"),\n",
    "            col(\"track_name\").alias(\"track_name\"),\n",
    "            col(\"artists\").alias(\"artist_name\"),\n",
    "            col(\"album_name\"),\n",
    "            col(\"popularity\").cast(\"integer\").alias(\"popularity\"),\n",
    "            col(\"duration_ms\").cast(\"long\").alias(\"duration_ms\"),\n",
    "            col(\"explicit\").cast(\"boolean\").alias(\"explicit\"),\n",
    "            col(\"danceability\").cast(\"double\").alias(\"danceability\"),\n",
    "            col(\"energy\").cast(\"double\").alias(\"energy\"),\n",
    "            col(\"key\").cast(\"integer\").alias(\"key\"),\n",
    "            col(\"loudness\").cast(\"double\").alias(\"loudness\"),\n",
    "            col(\"mode\").cast(\"integer\").alias(\"mode\"),\n",
    "            col(\"speechiness\").cast(\"double\").alias(\"speechiness\"),\n",
    "            col(\"acousticness\").cast(\"double\").alias(\"acousticness\"),\n",
    "            col(\"instrumentalness\").cast(\"double\").alias(\"instrumentalness\"),\n",
    "            col(\"liveness\").cast(\"double\").alias(\"liveness\"),\n",
    "            col(\"valence\").cast(\"double\").alias(\"valence\"),\n",
    "            col(\"tempo\").cast(\"double\").alias(\"tempo\")\n",
    "        )\n",
    "        \n",
    "        # Remove rows with null track_id or track_name\n",
    "        # TRUSTED ZONE\n",
    "        # processed_df = processed_df.filter(\n",
    "        #     col(\"track_id\").isNotNull() & \n",
    "        #     col(\"track_name\").isNotNull()\n",
    "        # )\n",
    "        \n",
    "        # Print schema and count after processing\n",
    "        logger.info(\"Processed schema:\")\n",
    "        processed_df.printSchema()\n",
    "        count_after = processed_df.count()\n",
    "        logger.info(f\"Count after processing: {count_after}\")\n",
    "        logger.info(f\"Removed {count_before - count_after} rows during processing\")\n",
    "        \n",
    "        # Write the processed data as Parquet\n",
    "        processed_df.write.mode(\"overwrite\").parquet(output_path)\n",
    "        logger.info(f\"Processed Spotify tracks data saved to {output_path}\")\n",
    "        \n",
    "        return output_path\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing Spotify tracks dataset: {e}\")\n",
    "        raise\n",
    "\n",
    "def process_top_songs(spark, input_path, output_path):\n",
    "    \"\"\"\n",
    "    Procesa el dataset de Spotify con el siguiente schema:\n",
    "    \n",
    "    root\n",
    "     |-- spotify_id: string (nullable = true)\n",
    "     |-- name: string (nullable = true)\n",
    "     |-- artists: string (nullable = true)\n",
    "     |-- daily_rank: string (nullable = true)\n",
    "     |-- daily_movement: string (nullable = true)\n",
    "     |-- weekly_movement: string (nullable = true)\n",
    "     |-- country: string (nullable = true)\n",
    "     |-- snapshot_date: string (nullable = true)\n",
    "     |-- popularity: string (nullable = true)\n",
    "     |-- is_explicit: string (nullable = true)\n",
    "     |-- duration_ms: string (nullable = true)\n",
    "     |-- album_name: string (nullable = true)\n",
    "     |-- album_release_date: string (nullable = true)\n",
    "     |-- danceability: string (nullable = true)\n",
    "     |-- energy: string (nullable = true)\n",
    "     |-- key: string (nullable = true)\n",
    "     |-- loudness: string (nullable = true)\n",
    "     |-- mode: string (nullable = true)\n",
    "     |-- speechiness: string (nullable = true)\n",
    "     |-- acousticness: double (nullable = true)\n",
    "     |-- instrumentalness: double (nullable = true)\n",
    "     |-- liveness: double (nullable = true)\n",
    "     |-- valence: double (nullable = true)\n",
    "     |-- tempo: double (nullable = true)\n",
    "     |-- time_signature: double (nullable = true)\n",
    "    \n",
    "    Args:\n",
    "        spark (SparkSession): La sesión de Spark.\n",
    "        input_path (str): Ruta del archivo CSV de entrada.\n",
    "        output_path (str): Ruta del directorio de salida en HDFS.\n",
    "    \"\"\"\n",
    "    logger.info(f\"Procesando datos de Spotify desde {input_path}\")\n",
    "    \n",
    "    try:\n",
    "        # Leer el archivo parquet\n",
    "        df = spark.read.parquet(input_path)\n",
    "        \n",
    "        # Imprimir schema y contar filas antes del procesamiento\n",
    "        logger.info(\"Schema original:\")\n",
    "        df.printSchema()\n",
    "        count_before = df.count()\n",
    "        logger.info(f\"Filas antes del procesamiento: {count_before}\")\n",
    "        \n",
    "        # Seleccionar y transformar las columnas según el nuevo schema\n",
    "        processed_df = df.select(\n",
    "            col(\"spotify_id\").alias(\"spotify_id\"),\n",
    "            col(\"name\").alias(\"track_name\"),        \n",
    "            col(\"artists\").alias(\"artist_name\"),\n",
    "            col(\"daily_rank\").cast(\"integer\").alias(\"daily_rank\"),\n",
    "            col(\"daily_movement\").alias(\"daily_movement\"),\n",
    "            col(\"weekly_movement\").alias(\"weekly_movement\"),\n",
    "            col(\"country\").alias(\"country\"),\n",
    "            col(\"snapshot_date\").alias(\"snapshot_date\"),\n",
    "            col(\"popularity\").cast(\"integer\").alias(\"popularity\"),\n",
    "            col(\"is_explicit\").alias(\"is_explicit\"),\n",
    "            col(\"duration_ms\").cast(\"long\").alias(\"duration_ms\"),\n",
    "            col(\"album_name\").alias(\"album_name\"),\n",
    "            col(\"album_release_date\").alias(\"album_release_date\"),\n",
    "            col(\"danceability\").alias(\"danceability\"),\n",
    "            col(\"energy\").alias(\"energy\"),\n",
    "            col(\"key\").alias(\"key\"),\n",
    "            col(\"loudness\").alias(\"loudness\"),\n",
    "            col(\"mode\").alias(\"mode\"),\n",
    "            col(\"speechiness\").alias(\"speechiness\"),\n",
    "            col(\"acousticness\").cast(\"double\").alias(\"acousticness\"),\n",
    "            col(\"instrumentalness\").cast(\"double\").alias(\"instrumentalness\"),\n",
    "            col(\"liveness\").cast(\"double\").alias(\"liveness\"),\n",
    "            col(\"valence\").cast(\"double\").alias(\"valence\"),\n",
    "            col(\"tempo\").cast(\"double\").alias(\"tempo\"),\n",
    "            col(\"time_signature\").cast(\"double\").alias(\"time_signature\")\n",
    "        )\n",
    "        \n",
    "        # Manejo de valores nulos para algunas columnas numéricas\n",
    "        # TRUSTED ZONE\n",
    "        # processed_df = processed_df.na.fill({\n",
    "        #     \"daily_rank\": 0,\n",
    "        #     \"popularity\": 0,\n",
    "        #     \"duration_ms\": 0,\n",
    "        #     \"acousticness\": 0.0,\n",
    "        #     \"instrumentalness\": 0.0,\n",
    "        #     \"liveness\": 0.0,\n",
    "        #     \"valence\": 0.0,\n",
    "        #     \"tempo\": 0.0,\n",
    "        #     \"time_signature\": 0.0\n",
    "        # })\n",
    "        \n",
    "        # Convertir snapshot_date y album_release_date a formato fecha (ajustar el patrón si es necesario)\n",
    "        processed_df = processed_df.withColumn(\"snapshot_date\", to_date(col(\"snapshot_date\"), \"yyyy-MM-dd\"))\n",
    "        processed_df = processed_df.withColumn(\"album_release_date\", to_date(col(\"album_release_date\"), \"yyyy-MM-dd\"))\n",
    "        \n",
    "        # Extraer año, mes y día a partir de snapshot_date para posibles análisis adicionales\n",
    "        processed_df = processed_df.withColumn(\"snapshot_year\", year(col(\"snapshot_date\"))) \\\n",
    "                                   .withColumn(\"snapshot_month\", month(col(\"snapshot_date\"))) \\\n",
    "                                   .withColumn(\"snapshot_day\", dayofmonth(col(\"snapshot_date\")))\n",
    "        \n",
    "        # Limpiar el campo country: eliminar espacios extras y convertir a mayúsculas\n",
    "        processed_df = processed_df.withColumn(\n",
    "            \"country\", \n",
    "            upper(trim(regexp_replace(col(\"country\"), \"\\\\s+\", \" \")))\n",
    "        )\n",
    "        \n",
    "        # Filtrar filas donde el nombre de la canción (track_name) no sea nulo\n",
    "        # TRUSTED ZONE\n",
    "        # processed_df = processed_df.filter(col(\"track_name\").isNotNull())\n",
    "        \n",
    "        # Imprimir el schema y contar las filas después del procesamiento\n",
    "        logger.info(\"Schema procesado:\")\n",
    "        processed_df.printSchema()\n",
    "        count_after = processed_df.count()\n",
    "        logger.info(f\"Filas después del procesamiento: {count_after}\")\n",
    "        logger.info(f\"Se removieron {count_before - count_after} filas durante el procesamiento\")\n",
    "        \n",
    "        # Escribir el DataFrame procesado en formato Parquet en HDFS\n",
    "        processed_df.write.mode(\"overwrite\").parquet(output_path)\n",
    "        logger.info(f\"Datos procesados de Spotify guardados en {output_path}\")\n",
    "        \n",
    "        return output_path\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error al procesar los datos de Spotify: {e}\")\n",
    "        raise\n",
    "\n",
    "def process_song_lyrics(spark, input_path, output_path):\n",
    "    \"\"\"\n",
    "    Process the song lyrics dataset.\n",
    "    \n",
    "    Args:\n",
    "        spark (SparkSession): The Spark session.\n",
    "        input_path (str): The input file path.\n",
    "        output_path (str): The output directory path.\n",
    "    \"\"\"\n",
    "    logger.info(f\"Processing song lyrics from {input_path}\")\n",
    "    \n",
    "    try:\n",
    "        # Read the CSV file with proper encoding\n",
    "        df = spark.read.parquet(input_path)\n",
    "        \n",
    "        # Print schema and count before processing\n",
    "        logger.info(\"Original schema:\")\n",
    "        df.printSchema()\n",
    "        count_before = df.count()\n",
    "        logger.info(f\"Count before processing: {count_before}\")\n",
    "        \n",
    "        # Clean and transform the data based on the actual columns in songs-lyrics.csv\n",
    "        processed_df = df.select(\n",
    "            col(\"Unnamed: 0\").cast(\"integer\").alias(\"song_id\"),\n",
    "            col(\"artist\").alias(\"artist_name\"),\n",
    "            col(\"song_name\").alias(\"song_name\"),\n",
    "            col(\"lyrics\").alias(\"song_lyrics\")\n",
    "        )\n",
    "        \n",
    "        # Clean artist and track names\n",
    "        processed_df = processed_df.withColumn(\"artist_name\", trim(col(\"artist_name\"))) \\\n",
    "                                 .withColumn(\"song_name\", trim(col(\"song_name\")))\n",
    "        \n",
    "        # Filter rows with valid song_id and track_name\n",
    "        # TRUSTED ZONE\n",
    "        # processed_df = processed_df.filter(col(\"song_id\").isNotNull() & col(\"track_name\").isNotNull())\n",
    "        \n",
    "        # Print schema and count after processing\n",
    "        logger.info(\"Processed schema:\")\n",
    "        processed_df.printSchema()\n",
    "        count_after = processed_df.count()\n",
    "        logger.info(f\"Count after processing: {count_after}\")\n",
    "        logger.info(f\"Removed {count_before - count_after} rows during processing\")\n",
    "        \n",
    "        # Write the processed data as Parquet\n",
    "        processed_df.write.mode(\"overwrite\").parquet(output_path)\n",
    "        logger.info(f\"Processed song lyrics data saved to {output_path}\")\n",
    "        \n",
    "        return output_path\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing song lyrics dataset: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input and output paths\n",
    "input_paths = {\n",
    "    'spotify_tracks': './data/landing_zone/spotify-tracks-dataset.parquet',\n",
    "    'top_songs': './data/landing_zone/top-spotify-songs-by-country.parquet',\n",
    "    'song_lyrics': './data/landing_zone/songs-lyrics.parquet'\n",
    "}\n",
    "output_paths = {\n",
    "    'spotify_tracks': './data/formatted_zone/spotify-tracks-dataset',\n",
    "    'top_songs': './data/formatted_zone/top-spotify-songs-by-country',\n",
    "    'song_lyrics': './data/formatted_zone/songs-lyrics'\n",
    "}\n",
    "\n",
    "# Process datasets\n",
    "process_spotify_tracks(spark, input_paths['spotify_tracks'], output_paths['spotify_tracks'])\n",
    "process_top_songs(spark, input_paths['top_songs'], output_paths['top_songs'])\n",
    "process_song_lyrics(spark, input_paths['song_lyrics'], output_paths['song_lyrics'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Trusted Zone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Exploitation Zone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Data Analysis Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
