{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Large-Scale Data Engineering for AI\n",
    "En aquest projecte hem treballat amb tres datasets relacionats amb la música, concretament amb cançons disponibles a Spotify. Cada dataset aporta una visió complementària de la música en streaming, des del contingut de les cançons fins a les metadades dels tracks i la seva popularitat per països. A continuació, es descriu breument cadascun:\n",
    "\n",
    "- **songs-lyrics** → Conté unes 25.000 cançons amb les seves respectives lletres.\n",
    "- **spotify-tracks-dataset** → Inclou informació de cançons de Spotify de 125 gèneres diferents i altres atributs musicals com la popularitat, l’energia, la dansabilitat, etc.\n",
    "- **top-spotify-songs-by-country** → Conté les cançons més escoltades diàriament a 72 països, i s’actualitza de manera contínua.\n",
    "\n",
    "## The Data Engineering Pipeline\n",
    "### The Landing Zone\n",
    "Pel que fa a la landing zone, hem utilitzat l'API de Kaggle per obtenir els datasets necessaris. Abans de poder-hi accedir, cal generar un token d'autenticació. Per fer-ho, cal accedir al perfil d’usuari de Kaggle, anar a la configuració (Settings) i, a la part inferior, fer clic a \"Create New Token\", la qual cosa descarregarà un fitxer .json amb les credencials.\n",
    "\n",
    "Ens hem basat en la [guia](https://www.kaggle.com/docs/api#getting-started-installation-&-authentication) oficial per instal·lar i configurar correctament l’API. Els passos bàsics són els següents: d'ús de l'API per aconseguir-ho.\n",
    "```\n",
    "pip install kaggle\n",
    "mkdir ~/.kaggle\n",
    "mv ~/Downloads/kaggle.json ~/.kaggle/kaggle.json\n",
    "```\n",
    "\n",
    "Un cop establerta la connexió amb l’API, hem creat una funció que automatitza la descàrrega dels tres datasets i els desa a la ubicació desitjada. Aquests datasets estan inicialment en format .csv, però hem optat per convertir-los a .parquet per optimitzar l'emmagatzematge i la lectura posterior.\n",
    "\n",
    "En concret, el dataset principal que conté les lletres també inclou informació addicional sobre àlbums i cançons. No obstant això, nosaltres només requerim el conjunt de dades corresponent a les lletres, així que descartem la resta.\n",
    "\n",
    "En resum, el procés consisteix en:\n",
    "\n",
    "- Descarregar els datasets des de l’API de Kaggle\n",
    "- Convertir-los a format .parquet\n",
    "- Guardar-los al directori data/landing_zone.\n",
    "\n",
    "Durant tot aquest procés es fan diverses comprovacions per garantir que, en cas d'error, es disposi de prou informació per detectar i solucionar el problema.\n",
    "\n",
    "A part, la funció encarregada de executar-ho, té un paràmetre anomenat update. Si aquest es fixa com a TRUE, només es descarregarà el dataset **top-spotify-songs-by-country**, ja que aquest és l'únic que es va actualizant cada cert temps (concretament, cada día). D'aquesta menera, el nostre **data collector** permet fer execucions periòdiques, i mantenir les dades actualitzades de manera senzilla."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IOStream.flush timed out\n",
      "Requirement already satisfied: kaggle in /home/violeta/.pyenv/versions/3.11.5/lib/python3.11/site-packages (1.7.4.2)\n",
      "Requirement already satisfied: bleach in /home/violeta/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from kaggle) (6.2.0)\n",
      "Requirement already satisfied: certifi>=14.05.14 in /home/violeta/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from kaggle) (2024.2.2)\n",
      "Requirement already satisfied: charset-normalizer in /home/violeta/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from kaggle) (3.3.2)\n",
      "Requirement already satisfied: idna in /home/violeta/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from kaggle) (3.6)\n",
      "Requirement already satisfied: protobuf in /home/violeta/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from kaggle) (4.25.3)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /home/violeta/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from kaggle) (2.8.2)\n",
      "Requirement already satisfied: python-slugify in /home/violeta/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from kaggle) (8.0.4)\n",
      "Requirement already satisfied: requests in /home/violeta/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from kaggle) (2.31.0)\n",
      "Requirement already satisfied: setuptools>=21.0.0 in /home/violeta/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from kaggle) (65.5.0)\n",
      "Requirement already satisfied: six>=1.10 in /home/violeta/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from kaggle) (1.16.0)\n",
      "Requirement already satisfied: text-unidecode in /home/violeta/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from kaggle) (1.3)\n",
      "Requirement already satisfied: tqdm in /home/violeta/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from kaggle) (4.66.2)\n",
      "Requirement already satisfied: urllib3>=1.15.1 in /home/violeta/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from kaggle) (2.2.1)\n",
      "Requirement already satisfied: webencodings in /home/violeta/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from kaggle) (0.5.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "IOStream.flush timed out\n",
      "Requirement already satisfied: pyspark in /home/violeta/.pyenv/versions/3.11.5/lib/python3.11/site-packages (3.5.5)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /home/violeta/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from pyspark) (0.10.9.7)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Imports and requirements\n",
    "!pip install kaggle\n",
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and requirements\n",
    "import os\n",
    "import kaggle\n",
    "import pandas as pd\n",
    "import logging\n",
    "import shutil\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lit, when, regexp_replace, trim, lower, upper, to_date, year, month, dayofmonth, explode, split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-13 00:53:51,241 - __main__ - INFO - Creating Spark session with app name: Spotify_ETL\n",
      "2025-04-13 00:53:51,380 - __main__ - INFO - Spark session created successfully\n"
     ]
    }
   ],
   "source": [
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Create Spark Session\n",
    "def create_spark_session(app_name=\"Spotify_ETL\"):\n",
    "    \"\"\"\n",
    "    Creates and returns a Spark session.\n",
    "    \n",
    "    Args:\n",
    "        app_name (str): The name of the Spark application.\n",
    "        \n",
    "    Returns:\n",
    "        SparkSession: A configured Spark session.\n",
    "    \"\"\"\n",
    "    logger.info(f\"Creating Spark session with app name: {app_name}\")\n",
    "    \n",
    "    try:\n",
    "        # Create a SparkSession with appropriate settings\n",
    "        spark = (SparkSession.builder\n",
    "                .appName(app_name)\n",
    "                .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "                .config(\"spark.sql.shuffle.partitions\", \"10\")\n",
    "                .config(\"spark.driver.memory\", \"2g\")\n",
    "                .config(\"spark.executor.memory\", \"4g\")\n",
    "                .config(\"spark.default.parallelism\", \"4\")\n",
    "                .config(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "                .getOrCreate())\n",
    "        \n",
    "        # Set log level to ERROR to reduce verbosity\n",
    "        spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "        \n",
    "        logger.info(\"Spark session created successfully\")\n",
    "        return spark\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error creating Spark session: {e}\")\n",
    "        raise\n",
    "\n",
    "spark = create_spark_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base directory and Landing Zone directory structure\n",
    "BASE_DIR = \"./data\"\n",
    "LANDING_ZONE_DIR = os.path.join(BASE_DIR, \"landing_zone\")\n",
    "os.makedirs(LANDING_ZONE_DIR, exist_ok=True)\n",
    "\n",
    "kaggle.api.authenticate()\n",
    "\n",
    "# List of datasets to ingest from Kaggle\n",
    "datasets = [\n",
    "        {\n",
    "            \"kaggle_id\": \"asaniczka/top-spotify-songs-in-73-countries-daily-updated\",\n",
    "            \"dataset_name\": \"top-spotify-songs-by-country\",\n",
    "            \"update\": True\n",
    "        },\n",
    "        {\n",
    "            \"kaggle_id\": \"maharshipandya/-spotify-tracks-dataset\",\n",
    "            \"dataset_name\": \"spotify-tracks-dataset\",\n",
    "            \"update\": False\n",
    "        },\n",
    "        {\n",
    "            \"kaggle_id\": \"terminate9298/songs-lyrics\",\n",
    "            \"dataset_name\": \"songs-lyrics\",\n",
    "            \"update\": False\n",
    "        }\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-12 23:04:22,233 - root - INFO - Starting the creation of the Landing Zone using Kaggle API\n",
      "2025-04-12 23:04:22,233 - root - INFO - Downloading dataset: asaniczka/top-spotify-songs-in-73-countries-daily-updated\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset URL: https://www.kaggle.com/datasets/asaniczka/top-spotify-songs-in-73-countries-daily-updated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-12 23:04:35,241 - root - INFO - CSV 'universal_top_spotify_songs.csv' converted to single Parquet file and saved as './data\\landing_zone\\top-spotify-songs-by-country.parquet'.\n",
      "2025-04-12 23:04:35,270 - root - INFO - Dataset 'top-spotify-songs-by-country' downloaded successfully.\n",
      "2025-04-12 23:04:35,319 - root - INFO - Dataset 'top-spotify-songs-by-country' processed successfully.\n",
      "2025-04-12 23:04:35,320 - root - INFO - Downloading dataset: maharshipandya/-spotify-tracks-dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset URL: https://www.kaggle.com/datasets/maharshipandya/-spotify-tracks-dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-12 23:04:37,349 - root - INFO - CSV 'dataset.csv' converted to single Parquet file and saved as './data\\landing_zone\\spotify-tracks-dataset.parquet'.\n",
      "2025-04-12 23:04:37,353 - root - INFO - Dataset 'spotify-tracks-dataset' downloaded successfully.\n",
      "2025-04-12 23:04:37,370 - root - INFO - Dataset 'spotify-tracks-dataset' processed successfully.\n",
      "2025-04-12 23:04:37,372 - root - INFO - Downloading dataset: terminate9298/songs-lyrics\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset URL: https://www.kaggle.com/datasets/terminate9298/songs-lyrics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-12 23:04:39,554 - root - INFO - CSV 'lyrics.csv' converted to single Parquet file and saved as './data\\landing_zone\\songs-lyrics.parquet'.\n",
      "2025-04-12 23:04:39,560 - root - INFO - Dataset 'songs-lyrics' downloaded successfully.\n",
      "2025-04-12 23:04:39,570 - root - INFO - Dataset 'songs-lyrics' processed successfully.\n",
      "2025-04-12 23:04:39,571 - root - INFO - All datasets have been processed.\n",
      "2025-04-12 23:04:39,572 - root - INFO - Landing Zone creation completed.\n"
     ]
    }
   ],
   "source": [
    "def data_collector_kaggle(kaggle_dataset: dict) -> None:\n",
    "    \"\"\"\n",
    "    Downloads a dataset from Kaggle and saves it to the landing zone.\n",
    "\n",
    "    Parameters:\n",
    "    kaggle_dataset (dict): A dictionary containing the Kaggle dataset information.\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract dataset information\n",
    "    kaggle_id = kaggle_dataset[\"kaggle_id\"]\n",
    "    dataset_name = kaggle_dataset[\"dataset_name\"]\n",
    "\n",
    "    # Create a temporary directory for the dataset using the actual dataset name\n",
    "    dataset_folder = os.path.join(LANDING_ZONE_DIR, f\"temp_{dataset_name}\")\n",
    "    os.makedirs(dataset_folder, exist_ok=True)\n",
    "\n",
    "    try:  \n",
    "        logging.info(f\"Downloading dataset: {kaggle_id}\")\n",
    "\n",
    "        kaggle.api.dataset_download_files(\n",
    "            kaggle_id,\n",
    "            path=dataset_folder,\n",
    "            unzip=True\n",
    "        )\n",
    "\n",
    "        csv_found = False\n",
    "        for filename in os.listdir(dataset_folder):\n",
    "            if filename in ['songs_details.csv', 'album_details.csv']:\n",
    "                continue\n",
    "            if filename.endswith(\".csv\"):\n",
    "                csv_found = True\n",
    "                csv_path = os.path.join(dataset_folder, filename)\n",
    "\n",
    "                # Read CSV with Pandas\n",
    "                df = pd.read_csv(csv_path)\n",
    "\n",
    "                # Write as a single Parquet file\n",
    "                final_path = os.path.join(LANDING_ZONE_DIR, f\"{dataset_name}.parquet\")\n",
    "                df.to_parquet(final_path, index=False)\n",
    "\n",
    "                logging.info(f\"CSV '{filename}' converted to single Parquet file and saved as '{final_path}'.\")\n",
    "                \n",
    "        if not csv_found:\n",
    "            logging.info(f\"No CSV file found in the downloaded dataset. Check the contents of the download.\")\n",
    "            \n",
    "        # Remove the temporary dataset folder\n",
    "        shutil.rmtree(dataset_folder)\n",
    "\n",
    "    except Exception as e:\n",
    "         # Remove the dataset folder if it exists\n",
    "        if os.path.exists(dataset_folder):\n",
    "            shutil.rmtree(dataset_folder)\n",
    "\n",
    "        # Log the error\n",
    "        logging.error(f\"Error downloading dataset '{kaggle_id}': {e}\")\n",
    "       \n",
    "        return\n",
    "\n",
    "    # Log the successful download\n",
    "    logging.info(f\"Dataset '{dataset_name}' downloaded successfully.\")\n",
    "\n",
    "def download_and_store_datasets(update: bool = False) -> None:\n",
    "    \"\"\"\n",
    "    Downloads and stores datasets from Kaggle into the landing zone.\n",
    "    \"\"\"\n",
    "    logging.info(\"Starting the creation of the Landing Zone using Kaggle API\")\n",
    "\n",
    "    for kaggle_dataset in datasets:\n",
    "        if update and not kaggle_dataset[\"update\"]:\n",
    "            logging.info(f\"Skipping dataset '{kaggle_dataset['dataset_name']}' as update is set to False.\")\n",
    "            continue\n",
    "        try:\n",
    "            dataset_name = kaggle_dataset[\"dataset_name\"]\n",
    "            data_collector_kaggle(kaggle_dataset)\n",
    "            logging.info(f\"Dataset '{dataset_name}' processed successfully.\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error processing dataset '{dataset_name}': {e}\")\n",
    "\n",
    "    logging.info(\"All datasets have been processed.\")\n",
    "    logging.info(\"Landing Zone creation completed.\")\n",
    "\n",
    "download_and_store_datasets(update=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Formatted Zone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_spotify_tracks(spark, input_path, output_path):\n",
    "    \"\"\"\n",
    "    Process the Spotify tracks dataset.\n",
    "    \n",
    "    Args:\n",
    "        spark (SparkSession): The Spark session.\n",
    "        input_path (str): The input file path.\n",
    "        output_path (str): The output directory path.\n",
    "    \"\"\"\n",
    "    logger.info(f\"Processing Spotify tracks dataset from {input_path}\")\n",
    "    \n",
    "    try:\n",
    "        # Read the parquet file\n",
    "        df = spark.read.parquet(input_path)\n",
    "        \n",
    "        # Print schema and count before processing\n",
    "        logger.info(\"Original schema:\")\n",
    "        df.printSchema()\n",
    "        count_before = df.count()\n",
    "        logger.info(f\"Count before processing: {count_before}\")\n",
    "        \n",
    "        # Clean and transform the data\n",
    "        processed_df = df.select(\n",
    "            col(\"track_id\").alias(\"track_id\"),\n",
    "            col(\"track_name\").alias(\"track_name\"),\n",
    "            col(\"artists\").alias(\"artist_name\"),\n",
    "            col(\"album_name\"),\n",
    "            col(\"popularity\").cast(\"integer\").alias(\"popularity\"),\n",
    "            col(\"duration_ms\").cast(\"long\").alias(\"duration_ms\"),\n",
    "            col(\"explicit\").cast(\"boolean\").alias(\"explicit\"),\n",
    "            col(\"danceability\").cast(\"double\").alias(\"danceability\"),\n",
    "            col(\"energy\").cast(\"double\").alias(\"energy\"),\n",
    "            col(\"key\").cast(\"integer\").alias(\"key\"),\n",
    "            col(\"loudness\").cast(\"double\").alias(\"loudness\"),\n",
    "            col(\"mode\").cast(\"integer\").alias(\"mode\"),\n",
    "            col(\"speechiness\").cast(\"double\").alias(\"speechiness\"),\n",
    "            col(\"acousticness\").cast(\"double\").alias(\"acousticness\"),\n",
    "            col(\"instrumentalness\").cast(\"double\").alias(\"instrumentalness\"),\n",
    "            col(\"liveness\").cast(\"double\").alias(\"liveness\"),\n",
    "            col(\"valence\").cast(\"double\").alias(\"valence\"),\n",
    "            col(\"tempo\").cast(\"double\").alias(\"tempo\")\n",
    "        )\n",
    "        \n",
    "        # Remove rows with null track_id or track_name\n",
    "        # TRUSTED ZONE\n",
    "        # processed_df = processed_df.filter(\n",
    "        #     col(\"track_id\").isNotNull() & \n",
    "        #     col(\"track_name\").isNotNull()\n",
    "        # )\n",
    "        \n",
    "        # Print schema and count after processing\n",
    "        logger.info(\"Processed schema:\")\n",
    "        processed_df.printSchema()\n",
    "        count_after = processed_df.count()\n",
    "        logger.info(f\"Count after processing: {count_after}\")\n",
    "        logger.info(f\"Removed {count_before - count_after} rows during processing\")\n",
    "        \n",
    "        # Write the processed data as Parquet\n",
    "        processed_df.write.mode(\"overwrite\").parquet(output_path)\n",
    "        logger.info(f\"Processed Spotify tracks data saved to {output_path}\")\n",
    "        \n",
    "        return output_path\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing Spotify tracks dataset: {e}\")\n",
    "        raise\n",
    "\n",
    "def process_top_songs(spark, input_path, output_path):\n",
    "    \"\"\"\n",
    "    Procesa el dataset de Spotify con el siguiente schema:\n",
    "    \n",
    "    root\n",
    "     |-- spotify_id: string (nullable = true)\n",
    "     |-- name: string (nullable = true)\n",
    "     |-- artists: string (nullable = true)\n",
    "     |-- daily_rank: string (nullable = true)\n",
    "     |-- daily_movement: string (nullable = true)\n",
    "     |-- weekly_movement: string (nullable = true)\n",
    "     |-- country: string (nullable = true)\n",
    "     |-- snapshot_date: string (nullable = true)\n",
    "     |-- popularity: string (nullable = true)\n",
    "     |-- is_explicit: string (nullable = true)\n",
    "     |-- duration_ms: string (nullable = true)\n",
    "     |-- album_name: string (nullable = true)\n",
    "     |-- album_release_date: string (nullable = true)\n",
    "     |-- danceability: string (nullable = true)\n",
    "     |-- energy: string (nullable = true)\n",
    "     |-- key: string (nullable = true)\n",
    "     |-- loudness: string (nullable = true)\n",
    "     |-- mode: string (nullable = true)\n",
    "     |-- speechiness: string (nullable = true)\n",
    "     |-- acousticness: double (nullable = true)\n",
    "     |-- instrumentalness: double (nullable = true)\n",
    "     |-- liveness: double (nullable = true)\n",
    "     |-- valence: double (nullable = true)\n",
    "     |-- tempo: double (nullable = true)\n",
    "     |-- time_signature: double (nullable = true)\n",
    "    \n",
    "    Args:\n",
    "        spark (SparkSession): La sesión de Spark.\n",
    "        input_path (str): Ruta del archivo CSV de entrada.\n",
    "        output_path (str): Ruta del directorio de salida en HDFS.\n",
    "    \"\"\"\n",
    "    logger.info(f\"Procesando datos de Spotify desde {input_path}\")\n",
    "    \n",
    "    try:\n",
    "        # Leer el archivo parquet\n",
    "        df = spark.read.parquet(input_path)\n",
    "        \n",
    "        # Imprimir schema y contar filas antes del procesamiento\n",
    "        logger.info(\"Schema original:\")\n",
    "        df.printSchema()\n",
    "        count_before = df.count()\n",
    "        logger.info(f\"Filas antes del procesamiento: {count_before}\")\n",
    "        \n",
    "        # Seleccionar y transformar las columnas según el nuevo schema\n",
    "        processed_df = df.select(\n",
    "            col(\"spotify_id\").alias(\"spotify_id\"),\n",
    "            col(\"name\").alias(\"track_name\"),        \n",
    "            col(\"artists\").alias(\"artist_name\"),\n",
    "            col(\"daily_rank\").cast(\"integer\").alias(\"daily_rank\"),\n",
    "            col(\"daily_movement\").alias(\"daily_movement\"),\n",
    "            col(\"weekly_movement\").alias(\"weekly_movement\"),\n",
    "            col(\"country\").alias(\"country\"),\n",
    "            col(\"snapshot_date\").alias(\"snapshot_date\"),\n",
    "            col(\"popularity\").cast(\"integer\").alias(\"popularity\"),\n",
    "            col(\"is_explicit\").alias(\"is_explicit\"),\n",
    "            col(\"duration_ms\").cast(\"long\").alias(\"duration_ms\"),\n",
    "            col(\"album_name\").alias(\"album_name\"),\n",
    "            col(\"album_release_date\").alias(\"album_release_date\"),\n",
    "            col(\"danceability\").alias(\"danceability\"),\n",
    "            col(\"energy\").alias(\"energy\"),\n",
    "            col(\"key\").alias(\"key\"),\n",
    "            col(\"loudness\").alias(\"loudness\"),\n",
    "            col(\"mode\").alias(\"mode\"),\n",
    "            col(\"speechiness\").alias(\"speechiness\"),\n",
    "            col(\"acousticness\").cast(\"double\").alias(\"acousticness\"),\n",
    "            col(\"instrumentalness\").cast(\"double\").alias(\"instrumentalness\"),\n",
    "            col(\"liveness\").cast(\"double\").alias(\"liveness\"),\n",
    "            col(\"valence\").cast(\"double\").alias(\"valence\"),\n",
    "            col(\"tempo\").cast(\"double\").alias(\"tempo\"),\n",
    "            col(\"time_signature\").cast(\"double\").alias(\"time_signature\")\n",
    "        )\n",
    "        \n",
    "        # Manejo de valores nulos para algunas columnas numéricas\n",
    "        # TRUSTED ZONE\n",
    "        # processed_df = processed_df.na.fill({\n",
    "        #     \"daily_rank\": 0,\n",
    "        #     \"popularity\": 0,\n",
    "        #     \"duration_ms\": 0,\n",
    "        #     \"acousticness\": 0.0,\n",
    "        #     \"instrumentalness\": 0.0,\n",
    "        #     \"liveness\": 0.0,\n",
    "        #     \"valence\": 0.0,\n",
    "        #     \"tempo\": 0.0,\n",
    "        #     \"time_signature\": 0.0\n",
    "        # })\n",
    "        \n",
    "        # Convertir snapshot_date y album_release_date a formato fecha (ajustar el patrón si es necesario)\n",
    "        processed_df = processed_df.withColumn(\"snapshot_date\", to_date(col(\"snapshot_date\"), \"yyyy-MM-dd\"))\n",
    "        processed_df = processed_df.withColumn(\"album_release_date\", to_date(col(\"album_release_date\"), \"yyyy-MM-dd\"))\n",
    "        \n",
    "        # Extraer año, mes y día a partir de snapshot_date para posibles análisis adicionales\n",
    "        processed_df = processed_df.withColumn(\"snapshot_year\", year(col(\"snapshot_date\"))) \\\n",
    "                                   .withColumn(\"snapshot_month\", month(col(\"snapshot_date\"))) \\\n",
    "                                   .withColumn(\"snapshot_day\", dayofmonth(col(\"snapshot_date\")))\n",
    "        \n",
    "        # Limpiar el campo country: eliminar espacios extras y convertir a mayúsculas\n",
    "        processed_df = processed_df.withColumn(\n",
    "            \"country\", \n",
    "            upper(trim(regexp_replace(col(\"country\"), \"\\\\s+\", \" \")))\n",
    "        )\n",
    "        \n",
    "        # Filtrar filas donde el nombre de la canción (track_name) no sea nulo\n",
    "        # TRUSTED ZONE\n",
    "        # processed_df = processed_df.filter(col(\"track_name\").isNotNull())\n",
    "        \n",
    "        # Imprimir el schema y contar las filas después del procesamiento\n",
    "        logger.info(\"Schema procesado:\")\n",
    "        processed_df.printSchema()\n",
    "        count_after = processed_df.count()\n",
    "        logger.info(f\"Filas después del procesamiento: {count_after}\")\n",
    "        logger.info(f\"Se removieron {count_before - count_after} filas durante el procesamiento\")\n",
    "        \n",
    "        # Escribir el DataFrame procesado en formato Parquet en HDFS\n",
    "        processed_df.write.mode(\"overwrite\").parquet(output_path)\n",
    "        logger.info(f\"Datos procesados de Spotify guardados en {output_path}\")\n",
    "        \n",
    "        return output_path\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error al procesar los datos de Spotify: {e}\")\n",
    "        raise\n",
    "\n",
    "def process_song_lyrics(spark, input_path, output_path):\n",
    "    \"\"\"\n",
    "    Process the song lyrics dataset.\n",
    "    \n",
    "    Args:\n",
    "        spark (SparkSession): The Spark session.\n",
    "        input_path (str): The input file path.\n",
    "        output_path (str): The output directory path.\n",
    "    \"\"\"\n",
    "    logger.info(f\"Processing song lyrics from {input_path}\")\n",
    "    \n",
    "    try:\n",
    "        # Read the CSV file with proper encoding\n",
    "        df = spark.read.parquet(input_path)\n",
    "        \n",
    "        # Print schema and count before processing\n",
    "        logger.info(\"Original schema:\")\n",
    "        df.printSchema()\n",
    "        count_before = df.count()\n",
    "        logger.info(f\"Count before processing: {count_before}\")\n",
    "        \n",
    "        # Clean and transform the data based on the actual columns in songs-lyrics.csv\n",
    "        processed_df = df.select(\n",
    "            col(\"Unnamed: 0\").cast(\"integer\").alias(\"song_id\"),\n",
    "            col(\"artist\").alias(\"artist_name\"),\n",
    "            col(\"song_name\").alias(\"song_name\"),\n",
    "            col(\"lyrics\").alias(\"song_lyrics\")\n",
    "        )\n",
    "        \n",
    "        # Clean artist and track names\n",
    "        processed_df = processed_df.withColumn(\"artist_name\", trim(col(\"artist_name\"))) \\\n",
    "                                 .withColumn(\"song_name\", trim(col(\"song_name\")))\n",
    "        \n",
    "        # Filter rows with valid song_id and track_name\n",
    "        # TRUSTED ZONE\n",
    "        # processed_df = processed_df.filter(col(\"song_id\").isNotNull() & col(\"track_name\").isNotNull())\n",
    "        \n",
    "        # Print schema and count after processing\n",
    "        logger.info(\"Processed schema:\")\n",
    "        processed_df.printSchema()\n",
    "        count_after = processed_df.count()\n",
    "        logger.info(f\"Count after processing: {count_after}\")\n",
    "        logger.info(f\"Removed {count_before - count_after} rows during processing\")\n",
    "        \n",
    "        # Write the processed data as Parquet\n",
    "        processed_df.write.mode(\"overwrite\").parquet(output_path)\n",
    "        logger.info(f\"Processed song lyrics data saved to {output_path}\")\n",
    "        \n",
    "        return output_path\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing song lyrics dataset: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-13 00:54:11,583 - __main__ - INFO - Processing Spotify tracks dataset from ./data/landing_zone/spotify-tracks-dataset.parquet\n",
      "2025-04-13 00:54:11,662 - __main__ - INFO - Original schema:\n",
      "2025-04-13 00:54:11,753 - __main__ - INFO - Count before processing: 114000\n",
      "2025-04-13 00:54:11,806 - __main__ - INFO - Processed schema:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Unnamed: 0: long (nullable = true)\n",
      " |-- track_id: string (nullable = true)\n",
      " |-- artists: string (nullable = true)\n",
      " |-- album_name: string (nullable = true)\n",
      " |-- track_name: string (nullable = true)\n",
      " |-- popularity: long (nullable = true)\n",
      " |-- duration_ms: long (nullable = true)\n",
      " |-- explicit: boolean (nullable = true)\n",
      " |-- danceability: double (nullable = true)\n",
      " |-- energy: double (nullable = true)\n",
      " |-- key: long (nullable = true)\n",
      " |-- loudness: double (nullable = true)\n",
      " |-- mode: long (nullable = true)\n",
      " |-- speechiness: double (nullable = true)\n",
      " |-- acousticness: double (nullable = true)\n",
      " |-- instrumentalness: double (nullable = true)\n",
      " |-- liveness: double (nullable = true)\n",
      " |-- valence: double (nullable = true)\n",
      " |-- tempo: double (nullable = true)\n",
      " |-- time_signature: long (nullable = true)\n",
      " |-- track_genre: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- track_id: string (nullable = true)\n",
      " |-- track_name: string (nullable = true)\n",
      " |-- artist_name: string (nullable = true)\n",
      " |-- album_name: string (nullable = true)\n",
      " |-- popularity: integer (nullable = true)\n",
      " |-- duration_ms: long (nullable = true)\n",
      " |-- explicit: boolean (nullable = true)\n",
      " |-- danceability: double (nullable = true)\n",
      " |-- energy: double (nullable = true)\n",
      " |-- key: integer (nullable = true)\n",
      " |-- loudness: double (nullable = true)\n",
      " |-- mode: integer (nullable = true)\n",
      " |-- speechiness: double (nullable = true)\n",
      " |-- acousticness: double (nullable = true)\n",
      " |-- instrumentalness: double (nullable = true)\n",
      " |-- liveness: double (nullable = true)\n",
      " |-- valence: double (nullable = true)\n",
      " |-- tempo: double (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-13 00:54:11,881 - __main__ - INFO - Count after processing: 114000\n",
      "2025-04-13 00:54:11,882 - __main__ - INFO - Removed 0 rows during processing\n",
      "2025-04-13 00:54:12,561 - __main__ - INFO - Processed Spotify tracks data saved to ./data/formatted_zone/spotify-tracks-dataset\n",
      "2025-04-13 00:54:12,562 - __main__ - INFO - Procesando datos de Spotify desde ./data/landing_zone/top-spotify-songs-by-country.parquet\n",
      "2025-04-13 00:54:12,624 - __main__ - INFO - Schema original:\n",
      "2025-04-13 00:54:12,715 - __main__ - INFO - Filas antes del procesamiento: 1923107\n",
      "2025-04-13 00:54:12,798 - __main__ - INFO - Schema procesado:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- spotify_id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- artists: string (nullable = true)\n",
      " |-- daily_rank: long (nullable = true)\n",
      " |-- daily_movement: long (nullable = true)\n",
      " |-- weekly_movement: long (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- snapshot_date: string (nullable = true)\n",
      " |-- popularity: long (nullable = true)\n",
      " |-- is_explicit: boolean (nullable = true)\n",
      " |-- duration_ms: long (nullable = true)\n",
      " |-- album_name: string (nullable = true)\n",
      " |-- album_release_date: string (nullable = true)\n",
      " |-- danceability: double (nullable = true)\n",
      " |-- energy: double (nullable = true)\n",
      " |-- key: long (nullable = true)\n",
      " |-- loudness: double (nullable = true)\n",
      " |-- mode: long (nullable = true)\n",
      " |-- speechiness: double (nullable = true)\n",
      " |-- acousticness: double (nullable = true)\n",
      " |-- instrumentalness: double (nullable = true)\n",
      " |-- liveness: double (nullable = true)\n",
      " |-- valence: double (nullable = true)\n",
      " |-- tempo: double (nullable = true)\n",
      " |-- time_signature: long (nullable = true)\n",
      "\n",
      "root\n",
      " |-- spotify_id: string (nullable = true)\n",
      " |-- track_name: string (nullable = true)\n",
      " |-- artist_name: string (nullable = true)\n",
      " |-- daily_rank: integer (nullable = true)\n",
      " |-- daily_movement: long (nullable = true)\n",
      " |-- weekly_movement: long (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- snapshot_date: date (nullable = true)\n",
      " |-- popularity: integer (nullable = true)\n",
      " |-- is_explicit: boolean (nullable = true)\n",
      " |-- duration_ms: long (nullable = true)\n",
      " |-- album_name: string (nullable = true)\n",
      " |-- album_release_date: date (nullable = true)\n",
      " |-- danceability: double (nullable = true)\n",
      " |-- energy: double (nullable = true)\n",
      " |-- key: long (nullable = true)\n",
      " |-- loudness: double (nullable = true)\n",
      " |-- mode: long (nullable = true)\n",
      " |-- speechiness: double (nullable = true)\n",
      " |-- acousticness: double (nullable = true)\n",
      " |-- instrumentalness: double (nullable = true)\n",
      " |-- liveness: double (nullable = true)\n",
      " |-- valence: double (nullable = true)\n",
      " |-- tempo: double (nullable = true)\n",
      " |-- time_signature: double (nullable = true)\n",
      " |-- snapshot_year: integer (nullable = true)\n",
      " |-- snapshot_month: integer (nullable = true)\n",
      " |-- snapshot_day: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-13 00:54:12,878 - __main__ - INFO - Filas después del procesamiento: 1923107\n",
      "2025-04-13 00:54:12,878 - __main__ - INFO - Se removieron 0 filas durante el procesamiento\n",
      "2025-04-13 00:54:18,083 - __main__ - INFO - Datos procesados de Spotify guardados en ./data/formatted_zone/top-spotify-songs-by-country\n",
      "2025-04-13 00:54:18,084 - __main__ - INFO - Processing song lyrics from ./data/landing_zone/songs-lyrics.parquet\n",
      "2025-04-13 00:54:18,212 - __main__ - INFO - Original schema:\n",
      "2025-04-13 00:54:18,346 - __main__ - INFO - Count before processing: 25742\n",
      "2025-04-13 00:54:18,378 - __main__ - INFO - Processed schema:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Unnamed: 0: long (nullable = true)\n",
      " |-- link: string (nullable = true)\n",
      " |-- artist: string (nullable = true)\n",
      " |-- song_name: string (nullable = true)\n",
      " |-- lyrics: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- song_id: integer (nullable = true)\n",
      " |-- artist_name: string (nullable = true)\n",
      " |-- song_name: string (nullable = true)\n",
      " |-- song_lyrics: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-13 00:54:18,497 - __main__ - INFO - Count after processing: 25742\n",
      "2025-04-13 00:54:18,498 - __main__ - INFO - Removed 0 rows during processing\n",
      "2025-04-13 00:54:18,988 - __main__ - INFO - Processed song lyrics data saved to ./data/formatted_zone/songs-lyrics\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'./data/formatted_zone/songs-lyrics'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define input and output paths\n",
    "input_paths = {\n",
    "    'spotify_tracks': './data/landing_zone/spotify-tracks-dataset.parquet',\n",
    "    'top_songs': './data/landing_zone/top-spotify-songs-by-country.parquet',\n",
    "    'song_lyrics': './data/landing_zone/songs-lyrics.parquet'\n",
    "}\n",
    "output_paths = {\n",
    "    'spotify_tracks': './data/formatted_zone/spotify-tracks-dataset',\n",
    "    'top_songs': './data/formatted_zone/top-spotify-songs-by-country',\n",
    "    'song_lyrics': './data/formatted_zone/songs-lyrics'\n",
    "}\n",
    "\n",
    "# Process datasets\n",
    "process_spotify_tracks(spark, input_paths['spotify_tracks'], output_paths['spotify_tracks'])\n",
    "process_top_songs(spark, input_paths['top_songs'], output_paths['top_songs'])\n",
    "process_song_lyrics(spark, input_paths['song_lyrics'], output_paths['song_lyrics'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Trusted Zone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Exploitation Zone\n",
    "\n",
    "En aquesta fase del procés, preparem i estructurem les dades finals per a ser utilitzades en aplicacions d'anàlisi o visualització. Treballem amb els datasets ja validats i netejats de la *trusted zone*, i generem dues sortides principals:\n",
    "\n",
    "**1. explicit_prediction**\n",
    "\n",
    "- Llegim les dades de lletres de cançons i de metadades dels tracks.\n",
    "- Unim les dades mitjançant Spark, fent un join basat en les columnes `artist_name` i `song_name`/`track_name`.\n",
    "- D'aquesta unió obtenim una estructura amb la lletra de la cançó i si aquesta és considerada explícita.\n",
    "- El resultat es desa en format Parquet a `data/exploitation_zone/explicit_prediction`.\n",
    "\n",
    "**2. data_visualization**\n",
    "\n",
    "- Carreguem el dataset de les cançons més escoltades per país.\n",
    "- Seleccionem només les columnes necessàries per a visualitzacions: `track_name`, `artist_name`, `daily_rank`, `snapshot_date`, i `country`.\n",
    "- El desem en format Parquet a `data/exploitation_zone/data_visualization`.\n",
    "\n",
    "Aquestes dues sortides constitueixen la base per a futurs models predictius i visualitzacions interactives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos cargados desde trusted_zone\n",
      "Registros en explicit_prediction: 2258\n",
      "Dataset explicit_prediction guardado correctamente\n",
      "Registros en data_visualization: 1923107\n",
      "Dataset data_visualization guardado correctamente\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# # Read all Parquet files in the folders\n",
    "# df_lyrics = spark.read.parquet(\"data/trusted_zone/songs-lyrics.parquet\")\n",
    "# rdd_lyrics = df_lyrics.withColumn(\"artist_name\", trim(regexp_replace(\"artist_name\", \" Lyrics\", \"\"))).rdd\n",
    "# rdd_tracks = spark.read.parquet(\"data/trusted_zone/spotify-tracks-dataset.parquet\").rdd\n",
    "\n",
    "# rdd_lyrics = rdd_lyrics.map(lambda f: ((f['artist_name'], f['song_name']), f['song_lyrics']))\n",
    "# rdd_tracks = rdd_tracks.map(lambda f: ((f['artist_name'], f['track_name']), f['explicit']))\n",
    "# rdd_joined = rdd_lyrics.join(rdd_tracks).map(lambda f: f[1])\n",
    "# df_explicit_prediction = spark.createDataFrame(rdd_joined)\n",
    "# df_explicit_prediction.write.parquet(\"data/exploitation_zone/explicit_prediction.parquet\")\n",
    "\n",
    "# rdd_top_songs = spark.read.parquet(\"data/trusted_zone/top-spotify-songs-by-country.parquet\").rdd\n",
    "# rdd_top_songs = rdd_top_songs.map(lambda f: (f['track_name'], f['artist_name'], f['daily_rank'], f['snapshot_date'], f['country']))\n",
    "# df_data_visualization = spark.createDataFrame(rdd_top_songs)\n",
    "# df_data_visualization.write.parquet(\"data/exploitation_zone/data_visualization.parquet\")\n",
    "\n",
    "\n",
    "# Leer los datos usando Spark\n",
    "try:\n",
    "    # Intentamos leer desde trusted_zone primero\n",
    "    df_lyrics = spark.read.parquet(\"data/trusted_zone/songs-lyrics\")\n",
    "    df_tracks = spark.read.parquet(\"data/trusted_zone/spotify-tracks-dataset\")\n",
    "    df_top_songs = spark.read.parquet(\"data/trusted_zone/top-spotify-songs-by-country\")\n",
    "    print(\"Datos cargados desde trusted_zone\")\n",
    "except Exception as e:\n",
    "    print(f\"Error al leer desde trusted_zone: {e}\")\n",
    "    print(\"Intentando leer desde landing_zone...\")\n",
    "    try:\n",
    "        # Si no podemos leer desde trusted_zone, leemos desde landing_zone\n",
    "        df_lyrics = spark.read.parquet(\"data/landing_zone/songs-lyrics\")\n",
    "        df_tracks = spark.read.parquet(\"data/landing_zone/spotify-tracks-dataset\")\n",
    "        df_top_songs = spark.read.parquet(\"data/landing_zone/top-spotify-songs-by-country\")\n",
    "        print(\"Datos cargados desde landing_zone\")\n",
    "        \n",
    "        # Adaptar nombres de columnas si es necesario\n",
    "        if 'artist' in df_lyrics.columns:\n",
    "            df_lyrics = df_lyrics.withColumnRenamed(\"artist\", \"artist_name\")\n",
    "            df_lyrics = df_lyrics.withColumnRenamed(\"lyrics\", \"song_lyrics\")\n",
    "        if 'name' in df_top_songs.columns:\n",
    "            df_top_songs = df_top_songs.withColumnRenamed(\"name\", \"track_name\")\n",
    "    except Exception as e2:\n",
    "        print(f\"Error al leer desde landing_zone: {e2}\")\n",
    "        print(\"No se pudieron cargar los datos.\")\n",
    "        raise\n",
    "\n",
    "# 1. Explicit prediction\n",
    "# Limpiar nombres de artistas si es necesario\n",
    "if 'artist_name' in df_lyrics.columns:\n",
    "    df_lyrics = df_lyrics.withColumn(\n",
    "        \"artist_name\", \n",
    "        regexp_replace(col(\"artist_name\"), \" Lyrics\", \"\")\n",
    "    )\n",
    "\n",
    "# Realizar join basado en artist_name y song_name/track_name\n",
    "try:\n",
    "    # Seleccionar columnas necesarias para el join\n",
    "    lyrics_for_join = df_lyrics.select(\"artist_name\", \"song_name\", \"song_lyrics\")\n",
    "    tracks_for_join = df_tracks.select(\"artist_name\", \"track_name\", \"explicit\")\n",
    "    \n",
    "    # Realizar join\n",
    "    explicit_prediction = lyrics_for_join.join(\n",
    "        tracks_for_join,\n",
    "        (lyrics_for_join.artist_name == tracks_for_join.artist_name) & \n",
    "        (lyrics_for_join.song_name == tracks_for_join.track_name),\n",
    "        \"inner\"\n",
    "    ).select(\n",
    "        lyrics_for_join.song_lyrics, \n",
    "        tracks_for_join.explicit\n",
    "    )\n",
    "    \n",
    "    # Mostrar datos resultantes\n",
    "    print(f\"Registros en explicit_prediction: {explicit_prediction.count()}\")\n",
    "    \n",
    "    # Guardar resultados\n",
    "    explicit_prediction.write.mode(\"overwrite\").parquet(\"data/exploitation_zone/explicit_prediction\")\n",
    "    print(\"Dataset explicit_prediction guardado correctamente\")\n",
    "except Exception as e:\n",
    "    print(f\"Error al crear explicit_prediction: {e}\")\n",
    "\n",
    "# 2. Data visualization\n",
    "try:\n",
    "    # Seleccionar columnas necesarias para visualización\n",
    "    data_visualization = df_top_songs.select(\n",
    "        \"track_name\", \"artist_name\", \"daily_rank\", \"snapshot_date\", \"country\"\n",
    "    )\n",
    "    \n",
    "    # Mostrar datos resultantes\n",
    "    print(f\"Registros en data_visualization: {data_visualization.count()}\")\n",
    "    \n",
    "    # Guardar resultados\n",
    "    data_visualization.write.mode(\"overwrite\").parquet(\"data/exploitation_zone/data_visualization\")\n",
    "    print(\"Dataset data_visualization guardado correctamente\")\n",
    "except Exception as e:\n",
    "    print(f\"Error al crear data_visualization: {e}\")\n",
    "\n",
    "# Detener la sesión Spark para liberar recursos\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Data Analysis Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
