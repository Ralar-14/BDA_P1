{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Large-Scale Data Engineering for AI\n",
    "En aquest projecte hem treballat amb tres datasets relacionats amb la música, concretament amb cançons disponibles a Spotify. Cada dataset aporta una visió complementària de la música en streaming, des del contingut de les cançons fins a les metadades dels tracks i la seva popularitat per països. A continuació, es descriu breument cadascun:\n",
    "\n",
    "- **songs-lyrics** → Conté unes 25.000 cançons amb les seves respectives lletres.\n",
    "- **spotify-tracks-dataset** → Inclou informació de cançons de Spotify de 125 gèneres diferents i altres atributs musicals com la popularitat, l’energia, la dansabilitat, etc.\n",
    "- **top-spotify-songs-by-country** → Conté les cançons més escoltades diàriament a 72 països, i s’actualitza de manera contínua.\n",
    "\n",
    "## The Data Engineering Pipeline\n",
    "### The Landing Zone\n",
    "Pel que fa a la landing zone, hem utilitzat l'API de Kaggle per obtenir els datasets necessaris. Abans de poder-hi accedir, cal generar un token d'autenticació. Per fer-ho, cal accedir al perfil d’usuari de Kaggle, anar a la configuració (Settings) i, a la part inferior, fer clic a \"Create New Token\", la qual cosa descarregarà un fitxer .json amb les credencials.\n",
    "\n",
    "Ens hem basat en la [guia](https://www.kaggle.com/docs/api#getting-started-installation-&-authentication) oficial per instal·lar i configurar correctament l’API. Els passos bàsics són els següents: d'ús de l'API per aconseguir-ho.\n",
    "```\n",
    "pip install kaggle\n",
    "mkdir ~/.kaggle\n",
    "mv ~/Downloads/kaggle.json ~/.kaggle/kaggle.json\n",
    "```\n",
    "\n",
    "Un cop establerta la connexió amb l’API, hem creat una funció que automatitza la descàrrega dels tres datasets i els desa a la ubicació desitjada. Aquests datasets estan inicialment en format .csv, però hem optat per convertir-los a .parquet per optimitzar l'emmagatzematge i la lectura posterior.\n",
    "\n",
    "En concret, el dataset principal que conté les lletres també inclou informació addicional sobre àlbums i cançons. No obstant això, nosaltres només requerim el conjunt de dades corresponent a les lletres, així que descartem la resta.\n",
    "\n",
    "En resum, el procés consisteix en:\n",
    "\n",
    "- Descarregar els datasets des de l’API de Kaggle\n",
    "- Convertir-los a format .parquet\n",
    "- Guardar-los al directori data/landing_zone.\n",
    "\n",
    "Durant tot aquest procés es fan diverses comprovacions per garantir que, en cas d'error, es disposi de prou informació per detectar i solucionar el problema.\n",
    "\n",
    "A part, la funció encarregada de executar-ho, té un paràmetre anomenat update. Si aquest es fixa com a TRUE, només es descarregarà el dataset **top-spotify-songs-by-country**, ja que aquest és l'únic que es va actualizant cada cert temps (concretament, cada día). D'aquesta menera, el nostre **data collector** permet fer execucions periòdiques, i mantenir les dades actualitzades de manera senzilla."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kaggle in c:\\users\\ralva\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (1.5.16)\n",
      "Requirement already satisfied: six>=1.10 in c:\\users\\ralva\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from kaggle) (1.16.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\ralva\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from kaggle) (2024.7.4)\n",
      "Requirement already satisfied: python-dateutil in c:\\users\\ralva\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from kaggle) (2.9.0.post0)\n",
      "Requirement already satisfied: requests in c:\\users\\ralva\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from kaggle) (2.32.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\ralva\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from kaggle) (4.66.5)\n",
      "Requirement already satisfied: python-slugify in c:\\users\\ralva\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from kaggle) (8.0.4)\n",
      "Requirement already satisfied: urllib3 in c:\\users\\ralva\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from kaggle) (2.2.2)\n",
      "Requirement already satisfied: bleach in c:\\users\\ralva\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from kaggle) (6.2.0)\n",
      "Requirement already satisfied: webencodings in c:\\users\\ralva\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from bleach->kaggle) (0.5.1)\n",
      "Requirement already satisfied: text-unidecode>=1.3 in c:\\users\\ralva\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from python-slugify->kaggle) (1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ralva\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests->kaggle) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ralva\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests->kaggle) (3.7)\n",
      "Requirement already satisfied: colorama in c:\\users\\ralva\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tqdm->kaggle) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.0.1\n",
      "[notice] To update, run: C:\\Users\\ralva\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in c:\\users\\ralva\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (3.4.1)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in c:\\users\\ralva\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pyspark) (0.10.9.7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.0.1\n",
      "[notice] To update, run: C:\\Users\\ralva\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting unidecode\n",
      "  Downloading Unidecode-1.3.8-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading Unidecode-1.3.8-py3-none-any.whl (235 kB)\n",
      "Installing collected packages: unidecode\n",
      "Successfully installed unidecode-1.3.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.0.1\n",
      "[notice] To update, run: C:\\Users\\ralva\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Imports and requirements\n",
    "!pip install kaggle\n",
    "!pip install pyspark\n",
    "!pip install unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and requirements\n",
    "import os\n",
    "import kaggle\n",
    "import pandas as pd\n",
    "import logging\n",
    "import shutil\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lit, when, regexp_replace, trim, lower, upper, to_date, year, month, dayofmonth, explode, split, row_number, udf\n",
    "from pyspark.sql.types import StringType, IntegerType, DateType, ArrayType, StructType, StructField\n",
    "from pyspark.sql import Window\n",
    "from unidecode import unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-13 01:39:13,865 - __main__ - INFO - Creating Spark session with app name: Spotify_ETL\n",
      "2025-04-13 01:39:18,810 - __main__ - INFO - Spark session created successfully\n"
     ]
    }
   ],
   "source": [
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Create Spark Session\n",
    "def create_spark_session(app_name=\"Spotify_ETL\"):\n",
    "    \"\"\"\n",
    "    Creates and returns a Spark session.\n",
    "    \n",
    "    Args:\n",
    "        app_name (str): The name of the Spark application.\n",
    "        \n",
    "    Returns:\n",
    "        SparkSession: A configured Spark session.\n",
    "    \"\"\"\n",
    "    logger.info(f\"Creating Spark session with app name: {app_name}\")\n",
    "    \n",
    "    try:\n",
    "        # Create a SparkSession with appropriate settings\n",
    "        spark = (SparkSession.builder\n",
    "                .appName(app_name)\n",
    "                .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "                .config(\"spark.sql.shuffle.partitions\", \"10\")\n",
    "                .config(\"spark.driver.memory\", \"2g\")\n",
    "                .config(\"spark.executor.memory\", \"4g\")\n",
    "                .config(\"spark.default.parallelism\", \"4\")\n",
    "                .config(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "                .getOrCreate())\n",
    "        \n",
    "        # Set log level to ERROR to reduce verbosity\n",
    "        spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "        \n",
    "        logger.info(\"Spark session created successfully\")\n",
    "        return spark\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error creating Spark session: {e}\")\n",
    "        raise\n",
    "\n",
    "spark = create_spark_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base directory and Landing Zone directory structure\n",
    "BASE_DIR = \"./data\"\n",
    "LANDING_ZONE_DIR = os.path.join(BASE_DIR, \"landing_zone\")\n",
    "os.makedirs(LANDING_ZONE_DIR, exist_ok=True)\n",
    "\n",
    "kaggle.api.authenticate()\n",
    "\n",
    "# List of datasets to ingest from Kaggle\n",
    "datasets = [\n",
    "        {\n",
    "            \"kaggle_id\": \"asaniczka/top-spotify-songs-in-73-countries-daily-updated\",\n",
    "            \"dataset_name\": \"top-spotify-songs-by-country\",\n",
    "            \"update\": True\n",
    "        },\n",
    "        {\n",
    "            \"kaggle_id\": \"maharshipandya/-spotify-tracks-dataset\",\n",
    "            \"dataset_name\": \"spotify-tracks-dataset\",\n",
    "            \"update\": False\n",
    "        },\n",
    "        {\n",
    "            \"kaggle_id\": \"terminate9298/songs-lyrics\",\n",
    "            \"dataset_name\": \"songs-lyrics\",\n",
    "            \"update\": False\n",
    "        }\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-13 01:39:24,264 - root - INFO - Starting the creation of the Landing Zone using Kaggle API\n",
      "2025-04-13 01:39:24,266 - root - INFO - Downloading dataset: asaniczka/top-spotify-songs-in-73-countries-daily-updated\n",
      "2025-04-13 01:39:39,027 - root - INFO - CSV 'universal_top_spotify_songs.csv' converted to single Parquet file and saved as './data\\landing_zone\\top-spotify-songs-by-country.parquet'.\n",
      "2025-04-13 01:39:39,075 - root - INFO - Dataset 'top-spotify-songs-by-country' downloaded successfully.\n",
      "2025-04-13 01:39:39,122 - root - INFO - Dataset 'top-spotify-songs-by-country' processed successfully.\n",
      "2025-04-13 01:39:39,124 - root - INFO - Downloading dataset: maharshipandya/-spotify-tracks-dataset\n",
      "2025-04-13 01:39:40,883 - root - INFO - CSV 'dataset.csv' converted to single Parquet file and saved as './data\\landing_zone\\spotify-tracks-dataset.parquet'.\n",
      "2025-04-13 01:39:40,887 - root - INFO - Dataset 'spotify-tracks-dataset' downloaded successfully.\n",
      "2025-04-13 01:39:40,896 - root - INFO - Dataset 'spotify-tracks-dataset' processed successfully.\n",
      "2025-04-13 01:39:40,897 - root - INFO - Downloading dataset: terminate9298/songs-lyrics\n",
      "2025-04-13 01:39:42,735 - root - INFO - CSV 'lyrics.csv' converted to single Parquet file and saved as './data\\landing_zone\\songs-lyrics.parquet'.\n",
      "2025-04-13 01:39:42,742 - root - INFO - Dataset 'songs-lyrics' downloaded successfully.\n",
      "2025-04-13 01:39:42,747 - root - INFO - Dataset 'songs-lyrics' processed successfully.\n",
      "2025-04-13 01:39:42,749 - root - INFO - All datasets have been processed.\n",
      "2025-04-13 01:39:42,749 - root - INFO - Landing Zone creation completed.\n"
     ]
    }
   ],
   "source": [
    "def data_collector_kaggle(kaggle_dataset: dict) -> None:\n",
    "    \"\"\"\n",
    "    Downloads a dataset from Kaggle and saves it to the landing zone.\n",
    "\n",
    "    Parameters:\n",
    "    kaggle_dataset (dict): A dictionary containing the Kaggle dataset information.\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract dataset information\n",
    "    kaggle_id = kaggle_dataset[\"kaggle_id\"]\n",
    "    dataset_name = kaggle_dataset[\"dataset_name\"]\n",
    "\n",
    "    # Create a temporary directory for the dataset using the actual dataset name\n",
    "    dataset_folder = os.path.join(LANDING_ZONE_DIR, f\"temp_{dataset_name}\")\n",
    "    os.makedirs(dataset_folder, exist_ok=True)\n",
    "\n",
    "    try:  \n",
    "        logging.info(f\"Downloading dataset: {kaggle_id}\")\n",
    "\n",
    "        kaggle.api.dataset_download_files(\n",
    "            kaggle_id,\n",
    "            path=dataset_folder,\n",
    "            unzip=True\n",
    "        )\n",
    "\n",
    "        csv_found = False\n",
    "        for filename in os.listdir(dataset_folder):\n",
    "            if filename in ['songs_details.csv', 'album_details.csv']:\n",
    "                continue\n",
    "            if filename.endswith(\".csv\"):\n",
    "                csv_found = True\n",
    "                csv_path = os.path.join(dataset_folder, filename)\n",
    "\n",
    "                # Read CSV with Pandas\n",
    "                df = pd.read_csv(csv_path)\n",
    "\n",
    "                # Write as a single Parquet file\n",
    "                final_path = os.path.join(LANDING_ZONE_DIR, f\"{dataset_name}.parquet\")\n",
    "                df.to_parquet(final_path, index=False)\n",
    "\n",
    "                logging.info(f\"CSV '{filename}' converted to single Parquet file and saved as '{final_path}'.\")\n",
    "                \n",
    "        if not csv_found:\n",
    "            logging.info(f\"No CSV file found in the downloaded dataset. Check the contents of the download.\")\n",
    "            \n",
    "        # Remove the temporary dataset folder\n",
    "        shutil.rmtree(dataset_folder)\n",
    "\n",
    "    except Exception as e:\n",
    "         # Remove the dataset folder if it exists\n",
    "        if os.path.exists(dataset_folder):\n",
    "            shutil.rmtree(dataset_folder)\n",
    "\n",
    "        # Log the error\n",
    "        logging.error(f\"Error downloading dataset '{kaggle_id}': {e}\")\n",
    "       \n",
    "        return\n",
    "\n",
    "    # Log the successful download\n",
    "    logging.info(f\"Dataset '{dataset_name}' downloaded successfully.\")\n",
    "\n",
    "def download_and_store_datasets(update: bool = False) -> None:\n",
    "    \"\"\"\n",
    "    Downloads and stores datasets from Kaggle into the landing zone.\n",
    "    \"\"\"\n",
    "    logging.info(\"Starting the creation of the Landing Zone using Kaggle API\")\n",
    "\n",
    "    for kaggle_dataset in datasets:\n",
    "        if update and not kaggle_dataset[\"update\"]:\n",
    "            logging.info(f\"Skipping dataset '{kaggle_dataset['dataset_name']}' as update is set to False.\")\n",
    "            continue\n",
    "        try:\n",
    "            dataset_name = kaggle_dataset[\"dataset_name\"]\n",
    "            data_collector_kaggle(kaggle_dataset)\n",
    "            logging.info(f\"Dataset '{dataset_name}' processed successfully.\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error processing dataset '{dataset_name}': {e}\")\n",
    "\n",
    "    logging.info(\"All datasets have been processed.\")\n",
    "    logging.info(\"Landing Zone creation completed.\")\n",
    "\n",
    "download_and_store_datasets(update=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### La Formatted Zone\n",
    "\n",
    "A la formatted zone, l'objectiu principal és transformar i netejar les dades descarregades a la landing zone per preparar-les per a anàlisis posteriors. Aquest procés inclou diverses operacions de preprocessament i validació per garantir la qualitat i consistència de les dades.\n",
    "\n",
    "#### Processos principals:\n",
    "\n",
    "1. **Conversió de formats**: Els datasets descarregats es guarden en format `.parquet` per optimitzar l'emmagatzematge i la lectura. Aquest format és més eficient en termes de compressió i velocitat de processament.\n",
    "\n",
    "2. **Normalització de noms de columnes**: Els noms de les columnes es normalitzen per assegurar la coherència i facilitar l'accés a les dades.\n",
    "\n",
    "3. **Filtrat i selecció de dades**: Es mantenen únicament les columnes rellevants.\n",
    "\n",
    "4. **Normalització i estandardització**: Els noms de cançons i artistes es normalitzen per evitar inconsistències causades per espais innecessaris.\n",
    "\n",
    "5. **Emmagatzematge**: Les dades processades es guarden al directori `data/formatted_zone` en format `.parquet`.\n",
    "\n",
    "En resum, el procés de la formatted zone consisteix en:\n",
    "\n",
    "- Convertir els datasets a `.parquet`.\n",
    "- Normalitzar els noms de les columnes.\n",
    "- Filtrar i seleccionar les dades rellevants.\n",
    "- Normalitzar les dades.\n",
    "- Guardar els resultats al directori `data/formatted_zone`.\n",
    "\n",
    "Aquest procés assegura que les dades estiguin netes, consistents i llestes per a l'anàlisi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir la UDF para aplicar unidecode a cada valor de la columna\n",
    "def unidecode_func(s):\n",
    "    return unidecode.unidecode(s) if s is not None else None\n",
    "\n",
    "unidecode_udf = udf(unidecode_func, StringType())\n",
    "\n",
    "def process_spotify_tracks(spark, input_path, output_path):\n",
    "    \"\"\"\n",
    "    Process the Spotify tracks dataset.\n",
    "    \n",
    "    Args:\n",
    "        spark (SparkSession): The Spark session.\n",
    "        input_path (str): The input file path.\n",
    "        output_path (str): The output directory path.\n",
    "    \"\"\"\n",
    "    logger.info(f\"Processing Spotify tracks dataset from {input_path}\")\n",
    "    \n",
    "    try:\n",
    "        # Read the parquet file\n",
    "        df = spark.read.parquet(input_path)\n",
    "        \n",
    "        # Print schema and count before processing\n",
    "        logger.info(\"Original schema:\")\n",
    "        df.printSchema()\n",
    "        count_before = df.count()\n",
    "        logger.info(f\"Count before processing: {count_before}\")\n",
    "        \n",
    "        # Clean and transform the data\n",
    "        processed_df = df.select(\n",
    "            col(\"track_id\").alias(\"track_id\"),\n",
    "            col(\"track_name\").alias(\"track_name\"),\n",
    "            col(\"artists\").alias(\"artist_name\"),\n",
    "            col(\"album_name\"),\n",
    "            col(\"popularity\").cast(\"integer\").alias(\"popularity\"),\n",
    "            col(\"duration_ms\").cast(\"long\").alias(\"duration_ms\"),\n",
    "            col(\"explicit\").cast(\"boolean\").alias(\"explicit\"),\n",
    "            col(\"danceability\").cast(\"double\").alias(\"danceability\"),\n",
    "            col(\"energy\").cast(\"double\").alias(\"energy\"),\n",
    "            col(\"key\").cast(\"integer\").alias(\"key\"),\n",
    "            col(\"loudness\").cast(\"double\").alias(\"loudness\"),\n",
    "            col(\"mode\").cast(\"integer\").alias(\"mode\"),\n",
    "            col(\"speechiness\").cast(\"double\").alias(\"speechiness\"),\n",
    "            col(\"acousticness\").cast(\"double\").alias(\"acousticness\"),\n",
    "            col(\"instrumentalness\").cast(\"double\").alias(\"instrumentalness\"),\n",
    "            col(\"liveness\").cast(\"double\").alias(\"liveness\"),\n",
    "            col(\"valence\").cast(\"double\").alias(\"valence\"),\n",
    "            col(\"tempo\").cast(\"double\").alias(\"tempo\")\n",
    "        )\n",
    "        \n",
    "        # Normalize track_name y artist_name usando la UDF de unidecode, trim y lower\n",
    "        processed_df = processed_df.withColumn(\"track_name\", lower(trim(col(\"track_name\"))))\n",
    "        processed_df = processed_df.withColumn(\"artist_name\", lower(trim(col(\"artist_name\"))))\n",
    "        \n",
    "        # Print schema and count after processing\n",
    "        logger.info(\"Processed schema:\")\n",
    "        processed_df.printSchema()\n",
    "        \n",
    "        # Write the processed data as Parquet\n",
    "        processed_df.write.mode(\"overwrite\").parquet(output_path)\n",
    "        logger.info(f\"Processed Spotify tracks data saved to {output_path}\")\n",
    "        \n",
    "        return output_path\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing Spotify tracks dataset: {e}\")\n",
    "        raise\n",
    "\n",
    "def process_top_songs(spark, input_path, output_path):\n",
    "    \"\"\"\n",
    "    Procesa el dataset de Spotify amb el següent schema:\n",
    "    \n",
    "    root\n",
    "     |-- spotify_id: string (nullable = true)\n",
    "     |-- name: string (nullable = true)\n",
    "     |-- artists: string (nullable = true)\n",
    "     |-- daily_rank: string (nullable = true)\n",
    "     |-- daily_movement: string (nullable = true)\n",
    "     |-- weekly_movement: string (nullable = true)\n",
    "     |-- country: string (nullable = true)\n",
    "     |-- snapshot_date: string (nullable = true)\n",
    "     |-- popularity: string (nullable = true)\n",
    "     |-- is_explicit: string (nullable = true)\n",
    "     |-- duration_ms: string (nullable = true)\n",
    "     |-- album_name: string (nullable = true)\n",
    "     |-- album_release_date: string (nullable = true)\n",
    "     |-- danceability: string (nullable = true)\n",
    "     |-- energy: string (nullable = true)\n",
    "     |-- key: string (nullable = true)\n",
    "     |-- loudness: string (nullable = true)\n",
    "     |-- mode: string (nullable = true)\n",
    "     |-- speechiness: string (nullable = true)\n",
    "     |-- acousticness: double (nullable = true)\n",
    "     |-- instrumentalness: double (nullable = true)\n",
    "     |-- liveness: double (nullable = true)\n",
    "     |-- valence: double (nullable = true)\n",
    "     |-- tempo: double (nullable = true)\n",
    "     |-- time_signature: double (nullable = true)\n",
    "    \n",
    "    Args:\n",
    "        spark (SparkSession): La sessió de Spark.\n",
    "        input_path (str): Ruta del fitxer CSV d'entrada.\n",
    "        output_path (str): Ruta del directori de sortida en HDFS.\n",
    "    \"\"\"\n",
    "    logger.info(f\"Processant dades de Spotify des de {input_path}\")\n",
    "    \n",
    "    try:\n",
    "        # Llegir el fitxer parquet\n",
    "        df = spark.read.parquet(input_path)\n",
    "        \n",
    "        # Imprimir schema i comptar files abans del processament\n",
    "        logger.info(\"Schema original:\")\n",
    "        df.printSchema()\n",
    "        count_before = df.count()\n",
    "        logger.info(f\"Files abans del processament: {count_before}\")\n",
    "        \n",
    "        # Seleccionar i transformar les columnes segons el nou schema\n",
    "        processed_df = df.select(\n",
    "            col(\"spotify_id\").alias(\"spotify_id\"),\n",
    "            col(\"name\").alias(\"track_name\"),        \n",
    "            col(\"artists\").alias(\"artist_name\"),\n",
    "            col(\"daily_rank\").cast(\"integer\").alias(\"daily_rank\"),\n",
    "            col(\"daily_movement\").alias(\"daily_movement\"),\n",
    "            col(\"weekly_movement\").alias(\"weekly_movement\"),\n",
    "            col(\"country\").alias(\"country\"),\n",
    "            col(\"snapshot_date\").alias(\"snapshot_date\"),\n",
    "            col(\"popularity\").cast(\"integer\").alias(\"popularity\"),\n",
    "            col(\"is_explicit\").alias(\"is_explicit\"),\n",
    "            col(\"duration_ms\").cast(\"long\").alias(\"duration_ms\"),\n",
    "            col(\"album_name\").alias(\"album_name\"),\n",
    "            col(\"album_release_date\").alias(\"album_release_date\"),\n",
    "            col(\"danceability\").alias(\"danceability\"),\n",
    "            col(\"energy\").alias(\"energy\"),\n",
    "            col(\"key\").alias(\"key\"),\n",
    "            col(\"loudness\").alias(\"loudness\"),\n",
    "            col(\"mode\").alias(\"mode\"),\n",
    "            col(\"speechiness\").alias(\"speechiness\"),\n",
    "            col(\"acousticness\").cast(\"double\").alias(\"acousticness\"),\n",
    "            col(\"instrumentalness\").cast(\"double\").alias(\"instrumentalness\"),\n",
    "            col(\"liveness\").cast(\"double\").alias(\"liveness\"),\n",
    "            col(\"valence\").cast(\"double\").alias(\"valence\"),\n",
    "            col(\"tempo\").cast(\"double\").alias(\"tempo\"),\n",
    "            col(\"time_signature\").cast(\"double\").alias(\"time_signature\")\n",
    "        )\n",
    "        \n",
    "        # Convertir snapshot_date i album_release_date a format data (ajustar el patró si és necessari)\n",
    "        processed_df = processed_df.withColumn(\"snapshot_date\", to_date(col(\"snapshot_date\"), \"yyyy-MM-dd\"))\n",
    "        processed_df = processed_df.withColumn(\"album_release_date\", to_date(col(\"album_release_date\"), \"yyyy-MM-dd\"))\n",
    "        \n",
    "        # Extreure any, mes i dia a partir de snapshot_date per a possibles anàlisis addicionals\n",
    "        processed_df = processed_df.withColumn(\"snapshot_year\", year(col(\"snapshot_date\"))) \\\n",
    "                                   .withColumn(\"snapshot_month\", month(col(\"snapshot_date\"))) \\\n",
    "                                   .withColumn(\"snapshot_day\", dayofmonth(col(\"snapshot_date\")))\n",
    "        \n",
    "        \n",
    "        # Imprimir el schema i comptar les files després del processament\n",
    "        logger.info(\"Schema processat:\")\n",
    "        processed_df.printSchema()\n",
    "        \n",
    "        # Escriure el DataFrame processat en format Parquet en HDFS\n",
    "        processed_df.write.mode(\"overwrite\").parquet(output_path)\n",
    "        logger.info(f\"Dades processades de Spotify guardades a {output_path}\")\n",
    "        \n",
    "        return output_path\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error al processar les dades de Spotify: {e}\")\n",
    "        raise\n",
    "\n",
    "def process_song_lyrics(spark, input_path, output_path):\n",
    "    \"\"\"\n",
    "    Process the song lyrics dataset.\n",
    "    \n",
    "    Args:\n",
    "        spark (SparkSession): The Spark session.\n",
    "        input_path (str): The input file path.\n",
    "        output_path (str): The output directory path.\n",
    "    \"\"\"\n",
    "    logger.info(f\"Processing song lyrics from {input_path}\")\n",
    "    \n",
    "    try:\n",
    "        # Read the CSV file with proper encoding\n",
    "        df = spark.read.parquet(input_path)\n",
    "        \n",
    "        # Print schema and count before processing\n",
    "        logger.info(\"Original schema:\")\n",
    "        df.printSchema()\n",
    "        count_before = df.count()\n",
    "        \n",
    "        # Clean and transform the data based on the actual columns in songs-lyrics.csv\n",
    "        processed_df = df.select(\n",
    "            col(\"Unnamed: 0\").cast(\"integer\").alias(\"song_id\"),\n",
    "            col(\"artist\").alias(\"artist_name\"),\n",
    "            col(\"song_name\").alias(\"track_name\"),\n",
    "            col(\"lyrics\").alias(\"song_lyrics\")\n",
    "        )\n",
    "        \n",
    "        # Clean artist and track names\n",
    "        processed_df = processed_df.withColumn(\"artist_name\", trim(col(\"artist_name\"))) \\\n",
    "                                 .withColumn(\"track_name\", trim(col(\"track_name\")))\n",
    "        \n",
    "        # Print schema and count after processing\n",
    "        logger.info(\"Processed schema:\")\n",
    "        processed_df.printSchema()\n",
    "        \n",
    "        # Write the processed data as Parquet\n",
    "        processed_df.write.mode(\"overwrite\").parquet(output_path)\n",
    "        logger.info(f\"Processed song lyrics data saved to {output_path}\")\n",
    "        \n",
    "        return output_path\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing song lyrics dataset: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-13 03:18:47,414 - __main__ - INFO - Processing Spotify tracks dataset from ./data/landing_zone/spotify-tracks-dataset.parquet\n",
      "2025-04-13 03:18:47,510 - __main__ - INFO - Original schema:\n",
      "2025-04-13 03:18:47,591 - __main__ - INFO - Count before processing: 114000\n",
      "2025-04-13 03:18:47,626 - __main__ - INFO - Processed schema:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Unnamed: 0: long (nullable = true)\n",
      " |-- track_id: string (nullable = true)\n",
      " |-- artists: string (nullable = true)\n",
      " |-- album_name: string (nullable = true)\n",
      " |-- track_name: string (nullable = true)\n",
      " |-- popularity: long (nullable = true)\n",
      " |-- duration_ms: long (nullable = true)\n",
      " |-- explicit: boolean (nullable = true)\n",
      " |-- danceability: double (nullable = true)\n",
      " |-- energy: double (nullable = true)\n",
      " |-- key: long (nullable = true)\n",
      " |-- loudness: double (nullable = true)\n",
      " |-- mode: long (nullable = true)\n",
      " |-- speechiness: double (nullable = true)\n",
      " |-- acousticness: double (nullable = true)\n",
      " |-- instrumentalness: double (nullable = true)\n",
      " |-- liveness: double (nullable = true)\n",
      " |-- valence: double (nullable = true)\n",
      " |-- tempo: double (nullable = true)\n",
      " |-- time_signature: long (nullable = true)\n",
      " |-- track_genre: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- track_id: string (nullable = true)\n",
      " |-- track_name: string (nullable = true)\n",
      " |-- artist_name: string (nullable = true)\n",
      " |-- album_name: string (nullable = true)\n",
      " |-- popularity: integer (nullable = true)\n",
      " |-- duration_ms: long (nullable = true)\n",
      " |-- explicit: boolean (nullable = true)\n",
      " |-- danceability: double (nullable = true)\n",
      " |-- energy: double (nullable = true)\n",
      " |-- key: integer (nullable = true)\n",
      " |-- loudness: double (nullable = true)\n",
      " |-- mode: integer (nullable = true)\n",
      " |-- speechiness: double (nullable = true)\n",
      " |-- acousticness: double (nullable = true)\n",
      " |-- instrumentalness: double (nullable = true)\n",
      " |-- liveness: double (nullable = true)\n",
      " |-- valence: double (nullable = true)\n",
      " |-- tempo: double (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-13 03:18:48,460 - __main__ - INFO - Processed Spotify tracks data saved to ./data/formatted_zone/spotify-tracks-dataset\n",
      "2025-04-13 03:18:48,461 - __main__ - INFO - Processant dades de Spotify des de ./data/landing_zone/top-spotify-songs-by-country.parquet\n",
      "2025-04-13 03:18:48,518 - __main__ - INFO - Schema original:\n",
      "2025-04-13 03:18:48,591 - __main__ - INFO - Files abans del processament: 1923107\n",
      "2025-04-13 03:18:48,653 - __main__ - INFO - Schema processat:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- spotify_id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- artists: string (nullable = true)\n",
      " |-- daily_rank: long (nullable = true)\n",
      " |-- daily_movement: long (nullable = true)\n",
      " |-- weekly_movement: long (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- snapshot_date: string (nullable = true)\n",
      " |-- popularity: long (nullable = true)\n",
      " |-- is_explicit: boolean (nullable = true)\n",
      " |-- duration_ms: long (nullable = true)\n",
      " |-- album_name: string (nullable = true)\n",
      " |-- album_release_date: string (nullable = true)\n",
      " |-- danceability: double (nullable = true)\n",
      " |-- energy: double (nullable = true)\n",
      " |-- key: long (nullable = true)\n",
      " |-- loudness: double (nullable = true)\n",
      " |-- mode: long (nullable = true)\n",
      " |-- speechiness: double (nullable = true)\n",
      " |-- acousticness: double (nullable = true)\n",
      " |-- instrumentalness: double (nullable = true)\n",
      " |-- liveness: double (nullable = true)\n",
      " |-- valence: double (nullable = true)\n",
      " |-- tempo: double (nullable = true)\n",
      " |-- time_signature: long (nullable = true)\n",
      "\n",
      "root\n",
      " |-- spotify_id: string (nullable = true)\n",
      " |-- track_name: string (nullable = true)\n",
      " |-- artist_name: string (nullable = true)\n",
      " |-- daily_rank: integer (nullable = true)\n",
      " |-- daily_movement: long (nullable = true)\n",
      " |-- weekly_movement: long (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- snapshot_date: date (nullable = true)\n",
      " |-- popularity: integer (nullable = true)\n",
      " |-- is_explicit: boolean (nullable = true)\n",
      " |-- duration_ms: long (nullable = true)\n",
      " |-- album_name: string (nullable = true)\n",
      " |-- album_release_date: date (nullable = true)\n",
      " |-- danceability: double (nullable = true)\n",
      " |-- energy: double (nullable = true)\n",
      " |-- key: long (nullable = true)\n",
      " |-- loudness: double (nullable = true)\n",
      " |-- mode: long (nullable = true)\n",
      " |-- speechiness: double (nullable = true)\n",
      " |-- acousticness: double (nullable = true)\n",
      " |-- instrumentalness: double (nullable = true)\n",
      " |-- liveness: double (nullable = true)\n",
      " |-- valence: double (nullable = true)\n",
      " |-- tempo: double (nullable = true)\n",
      " |-- time_signature: double (nullable = true)\n",
      " |-- snapshot_year: integer (nullable = true)\n",
      " |-- snapshot_month: integer (nullable = true)\n",
      " |-- snapshot_day: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-13 03:18:55,898 - __main__ - INFO - Dades processades de Spotify guardades a ./data/formatted_zone/top-spotify-songs-by-country\n",
      "2025-04-13 03:18:55,899 - __main__ - INFO - Processing song lyrics from ./data/landing_zone/songs-lyrics.parquet\n",
      "2025-04-13 03:18:55,959 - __main__ - INFO - Original schema:\n",
      "2025-04-13 03:18:56,049 - __main__ - INFO - Count before processing: 25742\n",
      "2025-04-13 03:18:56,063 - __main__ - INFO - Processed schema:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Unnamed: 0: long (nullable = true)\n",
      " |-- link: string (nullable = true)\n",
      " |-- artist: string (nullable = true)\n",
      " |-- song_name: string (nullable = true)\n",
      " |-- lyrics: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- song_id: integer (nullable = true)\n",
      " |-- artist_name: string (nullable = true)\n",
      " |-- track_name: string (nullable = true)\n",
      " |-- song_lyrics: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-13 03:18:56,502 - __main__ - INFO - Processed song lyrics data saved to ./data/formatted_zone/songs-lyrics\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'./data/formatted_zone/songs-lyrics'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define input and output paths\n",
    "landing_paths = {\n",
    "    'spotify_tracks': './data/landing_zone/spotify-tracks-dataset.parquet',\n",
    "    'top_songs': './data/landing_zone/top-spotify-songs-by-country.parquet',\n",
    "    'song_lyrics': './data/landing_zone/songs-lyrics.parquet'\n",
    "}\n",
    "formatted_paths = {\n",
    "    'spotify_tracks': './data/formatted_zone/spotify-tracks-dataset',\n",
    "    'top_songs': './data/formatted_zone/top-spotify-songs-by-country',\n",
    "    'song_lyrics': './data/formatted_zone/songs-lyrics'\n",
    "}\n",
    "\n",
    "# Process datasets\n",
    "process_spotify_tracks(spark, landing_paths['spotify_tracks'], formatted_paths['spotify_tracks'])\n",
    "process_top_songs(spark, landing_paths['top_songs'], formatted_paths['top_songs'])\n",
    "process_song_lyrics(spark, landing_paths['song_lyrics'], formatted_paths['song_lyrics'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### La Trusted Zone\n",
    "\n",
    "La trusted zone és la capa del pipeline de dades on les dades ja processades es validaden i es preparen pel futur anàlisis i visualitzacions. Aquesta zona conté dades enriquides habitualment amb cheques de qualitat i validacions per assegurar la seva integritat. En el nostre cas, hem realitzat diverses operacions per garantir que les dades siguin fiables i útils per a l'anàlisi posterior.\n",
    "\n",
    "#### Processos principals:\n",
    "\n",
    "1. **Filtrat de dades**: Es filtra el dataset per eliminar les cançons que no contenen lletres. Això és important perquè les lletres són un dels aspectes més rellevants del nostre projecte. També es filtra eliminan les cançons que no tenen album o nom de canço associat, ja que representen un error en el dataset.\n",
    "\n",
    "2. **Denial constraints**: S'apliquen restriccions per assegurar que les dades compleixin amb certes condicions. En el nostre cas, es verifiquen les següents condicions:\n",
    "    - **Durada mínima de les cançons**\n",
    "   \n",
    "        Cada cançó ha de tenir una duració de com a mínim 30 segons:\n",
    "\n",
    "        $$\n",
    "        \\forall r \\in R,\\; r.duration\\_ms \\ge 30000\n",
    "        $$\n",
    "\n",
    "    - **Unicitat del daily_rank per país i data**\n",
    "   \n",
    "   Dues cançons no poden compartir el mateix `daily_rank` en la mateixa data i país:\n",
    "\n",
    "   $$\n",
    "   \\forall r, s \\in R,\\; \\left[\n",
    "       \\left(\n",
    "           r.snapshot\\_date = s.snapshot\\_date \\land\n",
    "           r.country = s.country \\land\n",
    "           r.daily\\_rank = s.daily\\_rank\n",
    "       \\right) \\Rightarrow r = s\n",
    "   \\right]\n",
    "   $$\n",
    "\n",
    "    - **Unicitat del track_name per artista, país i data**\n",
    "   \n",
    "   Un artista no pot tenir dues cançons amb el mateix nom al mateix país i data:\n",
    "\n",
    "   $$\n",
    "   \\forall r, s \\in R,\\; \\left[\n",
    "       \\left(\n",
    "           r.artist\\_name = s.artist\\_name \\land\n",
    "           r.country = s.country \\land\n",
    "           r.snapshot\\_date = s.snapshot\\_date \\land\n",
    "           r.track\\_name = s.track\\_name\n",
    "       \\right) \\Rightarrow r = s\n",
    "   \\right]\n",
    "   $$\n",
    "\n",
    "    - **Unicitat del song_id**\n",
    "   \n",
    "   Cada `song_id` ha de ser únic dins el dataset de lletres:\n",
    "\n",
    "   $$\n",
    "   \\forall r, s \\in R,\\; \\left[\n",
    "       r.song\\_id = s.song\\_id \\Rightarrow r = s\n",
    "   \\right]\n",
    "   $$\n",
    "\n",
    "3. **Emmagatzematge**: Les dades validades es guarden al directori `data/trusted_zone` en format `.parquet`.\n",
    "\n",
    "En resum, el procés de la trusted zone consisteix en:\n",
    "\n",
    "- Filtrar les dades per eliminar cançons sense lletres o amb errors.\n",
    "- Aplicar restriccions per assegurar la qualitat de les dades.\n",
    "- Guardar els resultats al directori `data/trusted_zone`.\n",
    "\n",
    "Aquest procés assegura que les dades siguin fiables i útils per a l'anàlisi posterior, i proporciona una base sòlida per a la presa de decisions i la generació d'informes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_spotify_tracks(spark, input_path, output_path):\n",
    "    \"\"\"\n",
    "    Preprocess the Spotify tracks dataset for the Trusted Zone.\n",
    "    Args:\n",
    "        spark (SparkSession): The Spark session.\n",
    "        input_path (str): The input file path.\n",
    "        output_path (str): The output directory path.\n",
    "    \"\"\"\n",
    "    logger.info(f\"Preprocessing Spotify tracks dataset from {input_path}\")\n",
    "\n",
    "    try:\n",
    "        # Read the parquet file\n",
    "        df = spark.read.parquet(input_path)\n",
    "\n",
    "        # Clean and transform the data\n",
    "        processed_df = df.select(\n",
    "            col(\"track_id\"),\n",
    "            col(\"track_name\"),\n",
    "            col(\"artist_name\"),\n",
    "            col(\"album_name\"),\n",
    "            col(\"popularity\"),\n",
    "            col(\"duration_ms\"),\n",
    "            col(\"explicit\"),\n",
    "            col(\"danceability\"),\n",
    "            col(\"energy\"),\n",
    "            col(\"key\"),\n",
    "            col(\"loudness\"),\n",
    "            col(\"mode\"),\n",
    "            col(\"speechiness\"),\n",
    "            col(\"acousticness\"),\n",
    "            col(\"instrumentalness\"),\n",
    "            col(\"liveness\"),\n",
    "            col(\"valence\"),\n",
    "            col(\"tempo\")\n",
    "        ).filter(\n",
    "            col(\"track_id\").isNotNull() & \n",
    "            col(\"track_name\").isNotNull() & \n",
    "            col(\"artist_name\").isNotNull() & \n",
    "            col(\"explicit\").isNotNull() &\n",
    "            (col(\"duration_ms\") >= 30000) # Canciones de menos de 30 segundos no son válidas para nuestro análisis\n",
    "        )\n",
    "\n",
    "        # Write the processed data as Parquet\n",
    "        processed_df.write.mode(\"overwrite\").parquet(output_path)\n",
    "        logger.info(f\"Processed Spotify tracks data saved to {output_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error preprocessing Spotify tracks dataset: {e}\")\n",
    "        raise\n",
    "\n",
    "def preprocess_top_songs(spark, input_path, output_path):\n",
    "    \"\"\"\n",
    "    Preprocess the top songs dataset for the Trusted Zone.\n",
    "    Args:\n",
    "        spark (SparkSession): The Spark session.\n",
    "        input_path (str): The input file path.\n",
    "        output_path (str): The output directory path.\n",
    "    \"\"\"\n",
    "    logger.info(f\"Preprocessing top songs dataset from {input_path}\")\n",
    "\n",
    "    try:\n",
    "        # Read the parquet file\n",
    "        df = spark.read.parquet(input_path)\n",
    "\n",
    "        # Clean and transform the data\n",
    "        processed_df = df.select(\n",
    "            col(\"spotify_id\"),\n",
    "            col(\"track_name\"),\n",
    "            col(\"artist_name\"),\n",
    "            col(\"daily_rank\"),\n",
    "            col(\"country\"),\n",
    "            col(\"snapshot_date\"),\n",
    "            col(\"popularity\"),\n",
    "            col(\"duration_ms\"),\n",
    "            col(\"danceability\"),\n",
    "            col(\"energy\"),\n",
    "            col(\"acousticness\"),\n",
    "            col(\"valence\"),\n",
    "            col(\"tempo\")\n",
    "        ).filter(\n",
    "            col(\"track_name\").isNotNull() & col(\"artist_name\").isNotNull()\n",
    "        )\n",
    "\n",
    "        # No dos canciones pueden tener el mismo daily_rank en la misma fecha y país\n",
    "        window_spec = Window.partitionBy(\"snapshot_date\", \"country\", \"daily_rank\").orderBy(\"track_name\")\n",
    "        processed_df = processed_df.withColumn(\"row_number\", row_number().over(window_spec))\n",
    "        valid_df = processed_df.filter(col(\"row_number\") == 1).drop(\"row_number\")\n",
    "        removed_rows = processed_df.count() - valid_df.count()\n",
    "        logger.info(f\"Rows removed due to 'not two songs same on same rank' denial constraint: {removed_rows}\")\n",
    "\n",
    "        # No dos canciones por el mismo artista pueden tener el mismo track_name en el mismo país y fecha\n",
    "        window_spec_artist = Window.partitionBy(\"artist_name\", \"country\", \"track_name\", \"snapshot_date\").orderBy(\"track_name\")\n",
    "        processed_df = valid_df.withColumn(\"row_number_artist\", row_number().over(window_spec_artist))\n",
    "        valid_df = processed_df.filter(col(\"row_number_artist\") == 1).drop(\"row_number_artist\")\n",
    "        removed_rows_artist = processed_df.count() - valid_df.count()\n",
    "        logger.info(f\"Rows removed due to 'not two songs by same artist and same track name' constraint: {removed_rows_artist}\")\n",
    "\n",
    "        # Write the processed data as Parquet\n",
    "        valid_df.write.mode(\"overwrite\").parquet(output_path)\n",
    "        logger.info(f\"Processed top songs data saved to {output_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error preprocessing top songs dataset: {e}\")\n",
    "        raise\n",
    "\n",
    "def preprocess_song_lyrics(spark, input_path, output_path):\n",
    "    \"\"\"\n",
    "    Preprocess the song lyrics dataset for the Trusted Zone.\n",
    "    Args:\n",
    "        spark (SparkSession): The Spark session.\n",
    "        input_path (str): The input file path.\n",
    "        output_path (str): The output directory path.\n",
    "    \"\"\"\n",
    "    logger.info(f\"Preprocessing song lyrics dataset from {input_path}\")\n",
    "\n",
    "    try:\n",
    "        # Read the parquet file\n",
    "        df = spark.read.parquet(input_path)\n",
    "\n",
    "        # Clean and transform the data\n",
    "        processed_df = df.select(\n",
    "            col(\"song_id\"),\n",
    "            col(\"artist_name\"),\n",
    "            col(\"track_name\"),\n",
    "            col(\"song_lyrics\")\n",
    "        ).filter(\n",
    "            col(\"song_id\").isNotNull() & \n",
    "            col(\"track_name\").isNotNull() & \n",
    "            col(\"artist_name\").isNotNull() & \n",
    "            col(\"song_lyrics\").isNotNull()\n",
    "        )\n",
    "\n",
    "        # No dos registros deben tener el mismo song_id \n",
    "        window_spec_song = Window.partitionBy(\"song_id\").orderBy(\"artist_name\")\n",
    "        processed_df = processed_df.withColumn(\"row_number_song\", row_number().over(window_spec_song))\n",
    "        valid_df = processed_df.filter(col(\"row_number_song\") == 1).drop(\"row_number_song\")\n",
    "        removed_rows = processed_df.count() - valid_df.count()\n",
    "        logger.info(f\"Rows removed due to 'not unique song_id' denial constraint: {removed_rows}\")\n",
    "\n",
    "        # Write the processed data as Parquet\n",
    "        valid_df.write.mode(\"overwrite\").parquet(output_path)\n",
    "        logger.info(f\"Processed song lyrics data saved to {output_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error preprocessing song lyrics dataset: {e}\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-13 03:35:44,735 - __main__ - INFO - Preprocessing Spotify tracks dataset from ./data/formatted_zone/spotify-tracks-dataset\n",
      "2025-04-13 03:35:45,469 - __main__ - INFO - Processed Spotify tracks data saved to ./data/trusted_zone/spotify-tracks-dataset\n",
      "2025-04-13 03:35:45,470 - __main__ - INFO - Preprocessing top songs dataset from ./data/formatted_zone/top-spotify-songs-by-country\n",
      "2025-04-13 03:35:47,222 - __main__ - INFO - Rows removed due to 'not two songs same on same rank' denial constraint: 3642\n",
      "2025-04-13 03:35:51,476 - __main__ - INFO - Rows removed due to 'not two songs by same artist and same track name' constraint: 46\n",
      "2025-04-13 03:35:56,393 - __main__ - INFO - Processed top songs data saved to ./data/trusted_zone/top-spotify-songs-by-country\n",
      "2025-04-13 03:35:56,394 - __main__ - INFO - Preprocessing song lyrics dataset from ./data/formatted_zone/songs-lyrics\n",
      "2025-04-13 03:35:56,796 - __main__ - INFO - Rows removed due to 'not unique song_id' denial constraint: 0\n",
      "2025-04-13 03:35:57,427 - __main__ - INFO - Processed song lyrics data saved to ./data/trusted_zone/songs-lyrics\n"
     ]
    }
   ],
   "source": [
    "# Define input and output paths for Trusted Zone\n",
    "trusted_zone_paths = {\n",
    "    'spotify_tracks': './data/trusted_zone/spotify-tracks-dataset',\n",
    "    'top_songs': './data/trusted_zone/top-spotify-songs-by-country',\n",
    "    'song_lyrics': './data/trusted_zone/songs-lyrics'\n",
    "}\n",
    "\n",
    "# Execute preprocessing for Trusted Zone\n",
    "preprocess_spotify_tracks(spark, formatted_paths['spotify_tracks'], trusted_zone_paths['spotify_tracks'])\n",
    "preprocess_top_songs(spark, formatted_paths['top_songs'], trusted_zone_paths['top_songs'])\n",
    "preprocess_song_lyrics(spark, formatted_paths['song_lyrics'], trusted_zone_paths['song_lyrics'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Exploitation Zone\n",
    "\n",
    "En aquesta fase del procés, preparem i estructurem les dades finals per a ser utilitzades en aplicacions d'anàlisi o visualització. Treballem amb els datasets ja validats i netejats de la *trusted zone*, i generem dues sortides principals:\n",
    "\n",
    "**1. explicit_prediction**\n",
    "\n",
    "- Llegim les dades de lletres de cançons i de metadades dels tracks.\n",
    "- Unim les dades mitjançant Spark, fent un join basat en les columnes `artist_name` i `song_name`/`track_name`.\n",
    "- D'aquesta unió obtenim una estructura amb la lletra de la cançó i si aquesta és considerada explícita.\n",
    "- El resultat es desa en format Parquet a `data/exploitation_zone/explicit_prediction`.\n",
    "\n",
    "**2. data_visualization**\n",
    "\n",
    "- Carreguem el dataset de les cançons més escoltades per país.\n",
    "- Seleccionem només les columnes necessàries per a visualitzacions: `track_name`, `artist_name`, `daily_rank`, `snapshot_date`, i `country`.\n",
    "- El desem en format Parquet a `data/exploitation_zone/data_visualization`.\n",
    "\n",
    "Aquestes dues sortides constitueixen la base per a futurs models predictius i visualitzacions interactives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos cargados desde trusted_zone\n",
      "Registros en explicit_prediction: 2258\n",
      "Dataset explicit_prediction guardado correctamente\n",
      "Registros en data_visualization: 1923107\n",
      "Dataset data_visualization guardado correctamente\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# # Read all Parquet files in the folders\n",
    "# df_lyrics = spark.read.parquet(\"data/trusted_zone/songs-lyrics.parquet\")\n",
    "# rdd_lyrics = df_lyrics.withColumn(\"artist_name\", trim(regexp_replace(\"artist_name\", \" Lyrics\", \"\"))).rdd\n",
    "# rdd_tracks = spark.read.parquet(\"data/trusted_zone/spotify-tracks-dataset.parquet\").rdd\n",
    "\n",
    "# rdd_lyrics = rdd_lyrics.map(lambda f: ((f['artist_name'], f['song_name']), f['song_lyrics']))\n",
    "# rdd_tracks = rdd_tracks.map(lambda f: ((f['artist_name'], f['track_name']), f['explicit']))\n",
    "# rdd_joined = rdd_lyrics.join(rdd_tracks).map(lambda f: f[1])\n",
    "# df_explicit_prediction = spark.createDataFrame(rdd_joined)\n",
    "# df_explicit_prediction.write.parquet(\"data/exploitation_zone/explicit_prediction.parquet\")\n",
    "\n",
    "# rdd_top_songs = spark.read.parquet(\"data/trusted_zone/top-spotify-songs-by-country.parquet\").rdd\n",
    "# rdd_top_songs = rdd_top_songs.map(lambda f: (f['track_name'], f['artist_name'], f['daily_rank'], f['snapshot_date'], f['country']))\n",
    "# df_data_visualization = spark.createDataFrame(rdd_top_songs)\n",
    "# df_data_visualization.write.parquet(\"data/exploitation_zone/data_visualization.parquet\")\n",
    "\n",
    "\n",
    "# Leer los datos usando Spark\n",
    "try:\n",
    "    # Intentamos leer desde trusted_zone primero\n",
    "    df_lyrics = spark.read.parquet(\"data/trusted_zone/songs-lyrics\")\n",
    "    df_tracks = spark.read.parquet(\"data/trusted_zone/spotify-tracks-dataset\")\n",
    "    df_top_songs = spark.read.parquet(\"data/trusted_zone/top-spotify-songs-by-country\")\n",
    "    print(\"Datos cargados desde trusted_zone\")\n",
    "except Exception as e:\n",
    "    print(f\"Error al leer desde trusted_zone: {e}\")\n",
    "    print(\"Intentando leer desde landing_zone...\")\n",
    "    try:\n",
    "        # Si no podemos leer desde trusted_zone, leemos desde landing_zone\n",
    "        df_lyrics = spark.read.parquet(\"data/landing_zone/songs-lyrics\")\n",
    "        df_tracks = spark.read.parquet(\"data/landing_zone/spotify-tracks-dataset\")\n",
    "        df_top_songs = spark.read.parquet(\"data/landing_zone/top-spotify-songs-by-country\")\n",
    "        print(\"Datos cargados desde landing_zone\")\n",
    "        \n",
    "        # Adaptar nombres de columnas si es necesario\n",
    "        if 'artist' in df_lyrics.columns:\n",
    "            df_lyrics = df_lyrics.withColumnRenamed(\"artist\", \"artist_name\")\n",
    "            df_lyrics = df_lyrics.withColumnRenamed(\"lyrics\", \"song_lyrics\")\n",
    "        if 'name' in df_top_songs.columns:\n",
    "            df_top_songs = df_top_songs.withColumnRenamed(\"name\", \"track_name\")\n",
    "    except Exception as e2:\n",
    "        print(f\"Error al leer desde landing_zone: {e2}\")\n",
    "        print(\"No se pudieron cargar los datos.\")\n",
    "        raise\n",
    "\n",
    "# 1. Explicit prediction\n",
    "# Limpiar nombres de artistas si es necesario\n",
    "if 'artist_name' in df_lyrics.columns:\n",
    "    df_lyrics = df_lyrics.withColumn(\n",
    "        \"artist_name\", \n",
    "        regexp_replace(col(\"artist_name\"), \" Lyrics\", \"\")\n",
    "    )\n",
    "\n",
    "# Realizar join basado en artist_name y song_name/track_name\n",
    "try:\n",
    "    # Seleccionar columnas necesarias para el join\n",
    "    lyrics_for_join = df_lyrics.select(\"artist_name\", \"song_name\", \"song_lyrics\")\n",
    "    tracks_for_join = df_tracks.select(\"artist_name\", \"track_name\", \"explicit\")\n",
    "    \n",
    "    # Realizar join\n",
    "    explicit_prediction = lyrics_for_join.join(\n",
    "        tracks_for_join,\n",
    "        (lyrics_for_join.artist_name == tracks_for_join.artist_name) & \n",
    "        (lyrics_for_join.song_name == tracks_for_join.track_name),\n",
    "        \"inner\"\n",
    "    ).select(\n",
    "        lyrics_for_join.song_lyrics, \n",
    "        tracks_for_join.explicit\n",
    "    )\n",
    "    \n",
    "    # Mostrar datos resultantes\n",
    "    print(f\"Registros en explicit_prediction: {explicit_prediction.count()}\")\n",
    "    \n",
    "    # Guardar resultados\n",
    "    explicit_prediction.write.mode(\"overwrite\").parquet(\"data/exploitation_zone/explicit_prediction\")\n",
    "    print(\"Dataset explicit_prediction guardado correctamente\")\n",
    "except Exception as e:\n",
    "    print(f\"Error al crear explicit_prediction: {e}\")\n",
    "\n",
    "# 2. Data visualization\n",
    "try:\n",
    "    # Seleccionar columnas necesarias para visualización\n",
    "    data_visualization = df_top_songs.select(\n",
    "        \"track_name\", \"artist_name\", \"daily_rank\", \"snapshot_date\", \"country\"\n",
    "    )\n",
    "    \n",
    "    # Mostrar datos resultantes\n",
    "    print(f\"Registros en data_visualization: {data_visualization.count()}\")\n",
    "    \n",
    "    # Guardar resultados\n",
    "    data_visualization.write.mode(\"overwrite\").parquet(\"data/exploitation_zone/data_visualization\")\n",
    "    print(\"Dataset data_visualization guardado correctamente\")\n",
    "except Exception as e:\n",
    "    print(f\"Error al crear data_visualization: {e}\")\n",
    "\n",
    "# Detener la sesión Spark para liberar recursos\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Data Analysis Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
